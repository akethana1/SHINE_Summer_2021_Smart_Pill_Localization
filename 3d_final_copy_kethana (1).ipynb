{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3d_final_copy_kethana.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oR8RE3SdQEoF",
        "outputId": "dd53a56d-1f8d-4dec-e1d6-6410be660c2c"
      },
      "source": [
        "#importing packages and tools\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.multioutput import MultiOutputRegressor\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "\n",
        "from sklearn.ensemble import ExtraTreesRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.svm import LinearSVR\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "from keras.wrappers.scikit_learn import KerasRegressor\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import datasets, layers, models\n",
        "from pprint import pprint\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import sys\n",
        "print(sys.getrecursionlimit())\n",
        "sys.setrecursionlimit(10000)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IjbQw3NzQEoH"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iVGn0sTeQEoI"
      },
      "source": [
        "This section just reads the data and combines it into a single dataframe before randomly splitting into training and test sets (x - inputs, y- outputs).\n",
        "\n",
        "TODO:\n",
        "- the data extraction is hardcoded - need to come up with something more general for the nxn case. code to read data from files should work provided the correct file paths. \n",
        "- file paths here are hardcoded. not sure why the relative file paths were not working correctly. did not have time to fix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q0IDWqUqQEoI"
      },
      "source": [
        "#import data \n",
        "coil1 = pd.read_csv(\"/content/2x2-1.csv\", names=['Freq', 'Phase', 'Distance', 'x1','x2','x3','x4','y1','y2','y3','y4','z1','z2','z3','z4'], skiprows=1)\n",
        "coil2 = pd.read_csv(\"/content/2x2-2.csv\", names=['Freq', 'Phase', 'Distance', 'x1','x2','x3','x4','y1','y2','y3','y4','z1','z2','z3','z4'], skiprows=1)\n",
        "coil3 = pd.read_csv(\"/content/2x2-3.csv\", names=['Freq', 'Phase', 'Distance', 'x1','x2','x3','x4','y1','y2','y3','y4','z1','z2','z3','z4'], skiprows=1)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-VGnxeTQEoJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7904a2eb-cc17-4455-f46a-b27b7b2c57c5"
      },
      "source": [
        "#starting location\n",
        "x = 0\n",
        "y = 0\n",
        "z = 0\n",
        "points = coil1.shape[0]\n",
        "print(points)\n",
        "#x lines\n",
        "x1 = coil1[['Distance']]\n",
        "x1['y'] = 0\n",
        "x1['z'] = 0\n",
        "x1.columns = ['x', 'y', 'z']\n",
        "\n",
        "x2 = x1.copy()\n",
        "x2['y'] = x2['y'] + 60\n",
        "x3 = x1.copy()\n",
        "x3['z'] = x3['z'] + 60\n",
        "x4 = x2.copy()\n",
        "x4['z'] = x4['z'] + 60\n",
        "\n",
        "x1_concat = pd.concat([x1,coil1['x1'],coil2['x1'],coil3['x1']],axis=1)\n",
        "x1_concat.columns = ['x', 'y', 'z', 'coil1','coil2','coil3']\n",
        "\n",
        "x2_concat = pd.concat([x2,coil1['x2'],coil2['x2'],coil3['x2']],axis=1)\n",
        "x2_concat.columns = ['x', 'y', 'z', 'coil1','coil2','coil3']\n",
        "\n",
        "x3_concat = pd.concat([x3,coil1['x3'],coil2['x3'],coil3['x3']],axis=1)\n",
        "x3_concat.columns = ['x', 'y', 'z', 'coil1','coil2','coil3']\n",
        "\n",
        "x4_concat = pd.concat([x4,coil1['x4'],coil2['x4'],coil3['x4']],axis=1)\n",
        "x4_concat.columns = ['x', 'y', 'z', 'coil1','coil2','coil3']\n",
        "\n",
        "#y lines\n",
        "y1 = x1[['y']]\n",
        "y1['a'] = coil1[['Distance']]\n",
        "y1['b'] = 0\n",
        "y1.columns = ['x', 'y', 'z']\n",
        "print(y1.columns)\n",
        "\n",
        "y2 = y1.copy()\n",
        "y2['x'] = y2['x'] + 60\n",
        "y3 = y1.copy()\n",
        "y3['z'] = y3['z'] + 60\n",
        "y4 = y2.copy()\n",
        "y4['z'] = y4['z'] + 60\n",
        "\n",
        "y1_concat = pd.concat([y1,coil1['y1'],coil2['y1'],coil3['y1']],axis=1)\n",
        "y1_concat.columns = ['x', 'y', 'z', 'coil1','coil2','coil3']\n",
        "\n",
        "y2_concat = pd.concat([y2,coil1['y2'],coil2['y2'],coil3['y2']],axis=1)\n",
        "y2_concat.columns = ['x', 'y', 'z', 'coil1','coil2','coil3']\n",
        "\n",
        "y3_concat = pd.concat([y3,coil1['y3'],coil2['y3'],coil3['y3']],axis=1)\n",
        "y3_concat.columns = ['x', 'y', 'z', 'coil1','coil2','coil3']\n",
        "\n",
        "y4_concat = pd.concat([y4,coil1['y4'],coil2['y4'],coil3['y4']],axis=1)\n",
        "y4_concat.columns = ['x', 'y', 'z', 'coil1','coil2','coil3']\n",
        "\n",
        "#z lines\n",
        "z4 = x1[['y']]\n",
        "z4['a'] = 0\n",
        "z4['b'] = coil1[['Distance']]\n",
        "z4.columns = ['x', 'y', 'z']\n",
        "\n",
        "z1 = z4.copy()\n",
        "z1['x'] = z1['x'] + 60\n",
        "z3 = z4.copy()\n",
        "z3['y'] = z3['y'] + 60\n",
        "z2 = z3.copy()\n",
        "z2['x'] = z2['x'] + 60\n",
        "\n",
        "z1_concat = pd.concat([z1,coil1['z1'],coil2['z1'],coil3['z1']],axis=1)\n",
        "z1_concat.columns = ['x', 'y', 'z', 'coil1','coil2','coil3']\n",
        "\n",
        "z2_concat = pd.concat([z2,coil1['z2'],coil2['z2'],coil3['z2']],axis=1)\n",
        "z2_concat.columns = ['x', 'y', 'z', 'coil1','coil2','coil3']\n",
        "\n",
        "z3_concat = pd.concat([z3,coil1['z3'],coil2['z3'],coil3['z3']],axis=1)\n",
        "z3_concat.columns = ['x', 'y', 'z', 'coil1','coil2','coil3']\n",
        "\n",
        "z4_concat = pd.concat([z4,coil1['z4'],coil2['z4'],coil3['z4']],axis=1)\n",
        "z4_concat.columns = ['x', 'y', 'z', 'coil1','coil2','coil3']\n",
        "print(z4_concat)\n",
        "print(z4_concat.columns)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "101\n",
            "Index(['x', 'y', 'z'], dtype='object')\n",
            "     x  y          z     coil1     coil2     coil3\n",
            "0    0  0   0.000000  0.008356  0.008818  0.002750\n",
            "1    0  0   0.600004  0.008216  0.008874  0.002756\n",
            "2    0  0   1.200008  0.008099  0.008909  0.002762\n",
            "3    0  0   1.800012  0.008004  0.008923  0.002769\n",
            "4    0  0   2.400016  0.007910  0.008936  0.002775\n",
            "..  .. ..        ...       ...       ...       ...\n",
            "96   0  0  57.600384  0.002929  0.008910  0.002736\n",
            "97   0  0  58.200388  0.002899  0.008901  0.002728\n",
            "98   0  0  58.800392  0.002868  0.008892  0.002720\n",
            "99   0  0  59.400396  0.002838  0.008883  0.002712\n",
            "100  0  0  60.000400  0.002808  0.008874  0.002705\n",
            "\n",
            "[101 rows x 6 columns]\n",
            "Index(['x', 'y', 'z', 'coil1', 'coil2', 'coil3'], dtype='object')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgGpqnulQEoJ"
      },
      "source": [
        "df = pd.concat([x1_concat,x2_concat,x3_concat,x4_concat,\n",
        "                   y1_concat,y2_concat,y3_concat,y4_concat,\n",
        "                   z1_concat,z2_concat,z3_concat,z4_concat])"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W1dm5AbEQEoK"
      },
      "source": [
        "x_train, x_test, y_train, y_test = train_test_split(df[[\"coil1\",\"coil2\",\"coil3\"]], df[['x','y','z']], test_size=0.3)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ANCGOeEtgHzD",
        "outputId": "29e7caf8-2f10-4e32-890b-d0f2f8a1228f"
      },
      "source": [
        "print(x_train)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "        coil1     coil2     coil3\n",
            "32   0.009688  0.002836  0.003824\n",
            "36   0.005457  0.002817  0.009999\n",
            "96   0.002929  0.008910  0.002736\n",
            "31   0.009381  0.009111  0.003608\n",
            "48   0.004689  0.009466  0.002892\n",
            "..        ...       ...       ...\n",
            "100  0.002808  0.008874  0.002705\n",
            "58   0.002918  0.002893  0.005224\n",
            "46   0.002931  0.009202  0.004522\n",
            "89   0.003141  0.008970  0.002797\n",
            "28   0.009443  0.005874  0.002876\n",
            "\n",
            "[848 rows x 3 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kUNiBj_lgH5m",
        "outputId": "c8b732a1-7ad3-410e-fc3f-d24600d413c0"
      },
      "source": [
        "print(y_train)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "             x          y          z\n",
            "32   60.000000  19.200128   0.000000\n",
            "36   60.000000  60.000000  21.600144\n",
            "96    0.000000   0.000000  57.600384\n",
            "31    0.000000  18.600124   0.000000\n",
            "48    0.000000   0.000000  28.800192\n",
            "..         ...        ...        ...\n",
            "100   0.000000   0.000000  60.000400\n",
            "58   60.000000  34.800232  60.000000\n",
            "46    0.000000  27.600184  60.000000\n",
            "89    0.000000   0.000000  53.400356\n",
            "28   16.800112   0.000000   0.000000\n",
            "\n",
            "[848 rows x 3 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BsxQq68fQEoK"
      },
      "source": [
        "## Deep Neural Network - DNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3QnldHnQEoK"
      },
      "source": [
        "Defines model structure, trains and evaluates DNN. To change the model structure edit the define_multi_output_model function. Mse for training and test tests reported.\n",
        "\n",
        "Hyperparamters to tune:\n",
        "- number of layers\n",
        "- layer width\n",
        "- activation function\n",
        "- loss function\n",
        "- optimizer\n",
        "\n",
        "Note that this version of the model is not optimal. Need to play around with the above parameters more. As a starting point the model may not not be complex enough."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8X4MIDDQEoL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "edcd974b-08da-4102-82ce-d2a398f57e2f"
      },
      "source": [
        "#optimization DNN algorithm\n",
        "layer_widths = [50,60,70,80,90,100,110,120]\n",
        "activations = [\"relu\",\"tanh\"]\n",
        "layer_counts = [5,6,7,9,8,9,10,11,12]\n",
        "#variables to save most optimal case\n",
        "mse_actual1 = 100 #actual training mse\n",
        "mse_actual2 = 100 #actual testing mse \n",
        "accuracy_actual = 0\n",
        "mean_actual = 100 #takes mean to make sure we get best data \n",
        "layer_width_actual = 0\n",
        "activation_actual = \"\"\n",
        "layer_count_actual = 0\n",
        "for layer_width in layer_widths: #testing sample size of diff widths\n",
        "  #mse temp is 0 to compare mse vals and accuracy to get most optimal model output\n",
        "  mse_temp1 = 0 #training\n",
        "  mse_temp2 = 0 #testing\n",
        "  accuracy_temp = 0 #accuracy temp\n",
        "  for activation2 in activations: #testing with activation small sample size\n",
        "    for layer_count in layer_counts:\n",
        "      model = Sequential() #creates new model wherever there is a first change in a test case(last loop)\n",
        "      i = 0 #iterative value\n",
        "      while i <= layer_count: #iterates for all different counts of layers\n",
        "        if i == layer_count: #gets all layers added, includes count to compile model for each case\n",
        "          model.compile(loss='mse', optimizer='adam',  metrics=['accuracy']) #compiles model\n",
        "          model.summary()\n",
        "          model.fit(x_train,y_train,epochs=200,batch_size=10,verbose=1) #fits model\n",
        "          #training\n",
        "          prediction_train = model.predict(x_train)\n",
        "          prediction_test = model.predict(x_test)\n",
        "          dnn_MSE_train = mean_squared_error(y_train, prediction_train)\n",
        "          dnn_MSE_test = mean_squared_error(y_test, prediction_test)\n",
        "          #mse, accuracy printed\n",
        "          print(\"MSE on Training Data: \" + str(dnn_MSE_train))\n",
        "          print(\"MSE on Test Data: \" + str(dnn_MSE_test))\n",
        "          score = model.evaluate(x_test, y_test) #uses evaluate to get accuracy\n",
        "          print('Test loss (MSE):', score[0])\n",
        "          print('Test accuracy:', score[1])\n",
        "          #sets variables of current iteration to temp\n",
        "          mse_temp1 = dnn_MSE_train\n",
        "          mse_temp2 = dnn_MSE_test\n",
        "          accuracy_temp = score[1]\n",
        "          if mse_temp2 < mse_actual2: #if we get better results\n",
        "            if accuracy_temp > accuracy_actual: #if we have better accuracy\n",
        "              accuracy_actual = accuracy_temp #sets accuracy, we want most accurate model possible\n",
        "              mse_actual1 = dnn_MSE_train\n",
        "              mse_actual2 = dnn_MSE_test\n",
        "              #sets all variables so we can print at end to determine most optimal model\n",
        "              layer_width_actual = layer_width\n",
        "              activation_actual = activation2\n",
        "              layer_count_actual = layer_count\n",
        "              #prints optimal model results           \n",
        "              print(layer_width_actual)\n",
        "              print(activation_actual)\n",
        "              print(layer_count_actual)\n",
        "          i += 1\n",
        "        elif i == layer_count-1: #last layer\n",
        "          model.add(layers.Flatten()) #flattens data to get into single vector for last layer\n",
        "          model.add(Dense(3)) #since data is 3d vector, last layer is width of 3\n",
        "          i += 1\n",
        "        else: #all layers until last layer, so we count exactly how many layers\n",
        "          model.add(Dense(layer_width, input_dim = 3, kernel_initializer='he_uniform', activation=activation2))\n",
        "          i += 1\n",
        "#prints optimal model results           \n",
        "print(layer_width_actual)\n",
        "print(activation_actual)\n",
        "print(layer_count_actual)\n",
        "z = 0\n",
        "#builds optimal structure of model again \n",
        "while z <= layer_count_actual:\n",
        "    if z == layer_count_actual:\n",
        "      model.compile(loss='mse', optimizer='adam',  metrics=['accuracy']) #compiles model\n",
        "      model.fit(x_train,y_train,epochs=200,batch_size=10,verbose=1) #fits model\n",
        "      #training\n",
        "      prediction_train = model.predict(x_train)\n",
        "      prediction_test = model.predict(x_test)\n",
        "      dnn_MSE_train = mean_squared_error(y_train, prediction_train)\n",
        "      dnn_MSE_test = mean_squared_error(y_test, prediction_test)\n",
        "      #mse, accuracy printed\n",
        "      print(\"MSE on Training Data: \" + str(dnn_MSE_train))\n",
        "      print(\"MSE on Test Data: \" + str(dnn_MSE_test))\n",
        "      score = model.evaluate(x_test, y_test) #uses evaluate to get accuracy\n",
        "      print('Test loss (MSE):', score[0])\n",
        "      print('Test accuracy:', score[1]) \n",
        "      z += 1 \n",
        "    elif z == layer_count_actual-1: #acounts for last layer\n",
        "      model.add(layers.Flatten()) #flattens data to get into single vector for last layer\n",
        "      model.add(Dense(3)) #since data is 3d vector, last layer is width of 3 \n",
        "      z += 1\n",
        "    else: #adds optimal configurations for all layers\n",
        "      model.add(Dense(layer_width_actual, input_dim = 3, kernel_initializer='he_uniform', activation=activation_actual))\n",
        "      z += 1\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_5 (Dense)              (None, 50)                200       \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 50)                2550      \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 50)                2550      \n",
            "_________________________________________________________________\n",
            "dense_8 (Dense)              (None, 50)                2550      \n",
            "_________________________________________________________________\n",
            "module_wrapper_1 (ModuleWrap (None, 50)                0         \n",
            "_________________________________________________________________\n",
            "dense_9 (Dense)              (None, 3)                 153       \n",
            "=================================================================\n",
            "Total params: 8,003\n",
            "Trainable params: 8,003\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/200\n",
            "85/85 [==============================] - 1s 2ms/step - loss: 1565.0730 - accuracy: 0.3896\n",
            "Epoch 2/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 715.0547 - accuracy: 0.3481\n",
            "Epoch 3/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 703.7035 - accuracy: 0.4019\n",
            "Epoch 4/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 702.3414 - accuracy: 0.4322\n",
            "Epoch 5/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 701.1113 - accuracy: 0.2801\n",
            "Epoch 6/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 703.0448 - accuracy: 0.2703\n",
            "Epoch 7/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 689.1687 - accuracy: 0.2825\n",
            "Epoch 8/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 692.1031 - accuracy: 0.4107\n",
            "Epoch 9/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 686.8797 - accuracy: 0.3328\n",
            "Epoch 10/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 684.6360 - accuracy: 0.3249\n",
            "Epoch 11/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 682.7116 - accuracy: 0.2603\n",
            "Epoch 12/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 677.6228 - accuracy: 0.3404\n",
            "Epoch 13/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 672.1547 - accuracy: 0.2741\n",
            "Epoch 14/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 664.5385 - accuracy: 0.3465\n",
            "Epoch 15/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 650.6483 - accuracy: 0.2553\n",
            "Epoch 16/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 644.1753 - accuracy: 0.3270\n",
            "Epoch 17/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 623.9000 - accuracy: 0.2598\n",
            "Epoch 18/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 604.8935 - accuracy: 0.3451\n",
            "Epoch 19/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 572.1708 - accuracy: 0.4092\n",
            "Epoch 20/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 532.6913 - accuracy: 0.4304\n",
            "Epoch 21/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 496.1109 - accuracy: 0.3721\n",
            "Epoch 22/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 492.1553 - accuracy: 0.4066\n",
            "Epoch 23/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 487.5328 - accuracy: 0.3870\n",
            "Epoch 24/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 499.9655 - accuracy: 0.3501\n",
            "Epoch 25/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 496.3438 - accuracy: 0.4397\n",
            "Epoch 26/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 473.5561 - accuracy: 0.3523\n",
            "Epoch 27/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 496.3437 - accuracy: 0.4671\n",
            "Epoch 28/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 479.5201 - accuracy: 0.4774\n",
            "Epoch 29/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 475.0325 - accuracy: 0.3607\n",
            "Epoch 30/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 494.5179 - accuracy: 0.3933\n",
            "Epoch 31/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 490.1080 - accuracy: 0.3240\n",
            "Epoch 32/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 482.4100 - accuracy: 0.3279\n",
            "Epoch 33/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 482.2256 - accuracy: 0.3577\n",
            "Epoch 34/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 501.4077 - accuracy: 0.4670\n",
            "Epoch 35/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 486.8614 - accuracy: 0.3449\n",
            "Epoch 36/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 463.3212 - accuracy: 0.4042\n",
            "Epoch 37/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 473.9820 - accuracy: 0.4040\n",
            "Epoch 38/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 496.3052 - accuracy: 0.3678\n",
            "Epoch 39/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 470.4268 - accuracy: 0.4655\n",
            "Epoch 40/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 493.2698 - accuracy: 0.3791\n",
            "Epoch 41/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 476.1614 - accuracy: 0.4267\n",
            "Epoch 42/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 484.5210 - accuracy: 0.2903\n",
            "Epoch 43/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 486.4724 - accuracy: 0.3264\n",
            "Epoch 44/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 485.6852 - accuracy: 0.3922\n",
            "Epoch 45/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 492.1126 - accuracy: 0.3050\n",
            "Epoch 46/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 480.9276 - accuracy: 0.4549\n",
            "Epoch 47/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 490.2499 - accuracy: 0.2834\n",
            "Epoch 48/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 490.7608 - accuracy: 0.4523\n",
            "Epoch 49/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 500.7435 - accuracy: 0.3009\n",
            "Epoch 50/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 469.2420 - accuracy: 0.4251\n",
            "Epoch 51/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 477.8696 - accuracy: 0.3177\n",
            "Epoch 52/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 487.6339 - accuracy: 0.3241\n",
            "Epoch 53/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 488.0113 - accuracy: 0.2813\n",
            "Epoch 54/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 479.9501 - accuracy: 0.3124\n",
            "Epoch 55/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 488.4546 - accuracy: 0.3411\n",
            "Epoch 56/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 484.3191 - accuracy: 0.3551\n",
            "Epoch 57/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 492.9790 - accuracy: 0.3429\n",
            "Epoch 58/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 475.3380 - accuracy: 0.3821\n",
            "Epoch 59/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 472.4586 - accuracy: 0.4455\n",
            "Epoch 60/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 481.6814 - accuracy: 0.3807\n",
            "Epoch 61/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 470.0363 - accuracy: 0.3957\n",
            "Epoch 62/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 431.2294 - accuracy: 0.6395\n",
            "Epoch 63/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 404.0544 - accuracy: 0.6441\n",
            "Epoch 64/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 319.9543 - accuracy: 0.6306\n",
            "Epoch 65/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 273.3676 - accuracy: 0.6382\n",
            "Epoch 66/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 249.9323 - accuracy: 0.6587\n",
            "Epoch 67/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 255.5838 - accuracy: 0.6879\n",
            "Epoch 68/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 253.6219 - accuracy: 0.6516\n",
            "Epoch 69/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 241.4445 - accuracy: 0.6543\n",
            "Epoch 70/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 232.7767 - accuracy: 0.6661\n",
            "Epoch 71/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 234.8163 - accuracy: 0.6544\n",
            "Epoch 72/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 235.5607 - accuracy: 0.6613\n",
            "Epoch 73/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 226.1771 - accuracy: 0.6470\n",
            "Epoch 74/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 227.2851 - accuracy: 0.6587\n",
            "Epoch 75/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 225.4657 - accuracy: 0.6854\n",
            "Epoch 76/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 233.1680 - accuracy: 0.6663\n",
            "Epoch 77/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 209.3857 - accuracy: 0.6550\n",
            "Epoch 78/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 217.3016 - accuracy: 0.6735\n",
            "Epoch 79/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 217.7324 - accuracy: 0.6718\n",
            "Epoch 80/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 214.1277 - accuracy: 0.6686\n",
            "Epoch 81/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 220.0055 - accuracy: 0.6565\n",
            "Epoch 82/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 226.2833 - accuracy: 0.6905\n",
            "Epoch 83/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 210.8225 - accuracy: 0.6764\n",
            "Epoch 84/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 215.7264 - accuracy: 0.7022\n",
            "Epoch 85/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 206.6230 - accuracy: 0.6859\n",
            "Epoch 86/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 197.1533 - accuracy: 0.7255\n",
            "Epoch 87/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 191.6754 - accuracy: 0.7060\n",
            "Epoch 88/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 186.1871 - accuracy: 0.7324\n",
            "Epoch 89/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 187.1715 - accuracy: 0.7227\n",
            "Epoch 90/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 188.5721 - accuracy: 0.7473\n",
            "Epoch 91/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 178.7560 - accuracy: 0.7305\n",
            "Epoch 92/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 178.8748 - accuracy: 0.7649\n",
            "Epoch 93/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 149.6829 - accuracy: 0.7502\n",
            "Epoch 94/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 134.2261 - accuracy: 0.7685\n",
            "Epoch 95/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 109.9848 - accuracy: 0.7996\n",
            "Epoch 96/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 76.2602 - accuracy: 0.7836\n",
            "Epoch 97/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 60.9370 - accuracy: 0.7833\n",
            "Epoch 98/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 42.5095 - accuracy: 0.7863\n",
            "Epoch 99/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 35.4539 - accuracy: 0.8166\n",
            "Epoch 100/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 29.6271 - accuracy: 0.8054\n",
            "Epoch 101/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 25.0303 - accuracy: 0.8356\n",
            "Epoch 102/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 23.0771 - accuracy: 0.8199\n",
            "Epoch 103/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 23.1856 - accuracy: 0.8285\n",
            "Epoch 104/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 22.2083 - accuracy: 0.8114\n",
            "Epoch 105/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 19.7226 - accuracy: 0.8217\n",
            "Epoch 106/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 18.9438 - accuracy: 0.8552\n",
            "Epoch 107/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 19.2135 - accuracy: 0.8315\n",
            "Epoch 108/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 18.6794 - accuracy: 0.8259\n",
            "Epoch 109/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 17.1016 - accuracy: 0.8479\n",
            "Epoch 110/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 18.3923 - accuracy: 0.8413\n",
            "Epoch 111/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 17.2236 - accuracy: 0.8274\n",
            "Epoch 112/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 17.3321 - accuracy: 0.8360\n",
            "Epoch 113/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 15.2626 - accuracy: 0.8321\n",
            "Epoch 114/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 15.3657 - accuracy: 0.8392\n",
            "Epoch 115/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 15.2873 - accuracy: 0.8428\n",
            "Epoch 116/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 15.2092 - accuracy: 0.8413\n",
            "Epoch 117/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 12.9841 - accuracy: 0.8618\n",
            "Epoch 118/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 14.0134 - accuracy: 0.8576\n",
            "Epoch 119/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 13.2721 - accuracy: 0.8624\n",
            "Epoch 120/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 12.8513 - accuracy: 0.8398\n",
            "Epoch 121/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 11.6534 - accuracy: 0.8462\n",
            "Epoch 122/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 11.6382 - accuracy: 0.8318\n",
            "Epoch 123/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 11.7825 - accuracy: 0.8245\n",
            "Epoch 124/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 12.0552 - accuracy: 0.8489\n",
            "Epoch 125/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 10.9447 - accuracy: 0.8726\n",
            "Epoch 126/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 9.4608 - accuracy: 0.8551\n",
            "Epoch 127/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 11.1017 - accuracy: 0.8321\n",
            "Epoch 128/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 11.6983 - accuracy: 0.8450\n",
            "Epoch 129/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 10.6748 - accuracy: 0.8474\n",
            "Epoch 130/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 9.4715 - accuracy: 0.8425\n",
            "Epoch 131/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 9.2179 - accuracy: 0.8556\n",
            "Epoch 132/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 9.0982 - accuracy: 0.8428\n",
            "Epoch 133/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 8.0026 - accuracy: 0.8756\n",
            "Epoch 134/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 7.9933 - accuracy: 0.8633\n",
            "Epoch 135/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 8.8169 - accuracy: 0.8358\n",
            "Epoch 136/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 7.7791 - accuracy: 0.8665\n",
            "Epoch 137/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 7.5545 - accuracy: 0.8389\n",
            "Epoch 138/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 6.8509 - accuracy: 0.8563\n",
            "Epoch 139/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 7.2480 - accuracy: 0.8308\n",
            "Epoch 140/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 7.3102 - accuracy: 0.8438\n",
            "Epoch 141/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 6.2446 - accuracy: 0.8416\n",
            "Epoch 142/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 5.8118 - accuracy: 0.8385\n",
            "Epoch 143/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 5.8062 - accuracy: 0.8447\n",
            "Epoch 144/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 5.5780 - accuracy: 0.8441\n",
            "Epoch 145/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 6.0736 - accuracy: 0.8619\n",
            "Epoch 146/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 4.8642 - accuracy: 0.8684\n",
            "Epoch 147/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 4.7595 - accuracy: 0.8483\n",
            "Epoch 148/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 4.8384 - accuracy: 0.8350\n",
            "Epoch 149/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 4.8141 - accuracy: 0.8375\n",
            "Epoch 150/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 5.1154 - accuracy: 0.8193\n",
            "Epoch 151/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 4.7585 - accuracy: 0.8312\n",
            "Epoch 152/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 4.1911 - accuracy: 0.8445\n",
            "Epoch 153/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 4.8584 - accuracy: 0.8523\n",
            "Epoch 154/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 3.8878 - accuracy: 0.8500\n",
            "Epoch 155/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 4.1165 - accuracy: 0.8551\n",
            "Epoch 156/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 4.0357 - accuracy: 0.8510\n",
            "Epoch 157/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 4.1166 - accuracy: 0.8508\n",
            "Epoch 158/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 3.9561 - accuracy: 0.8299\n",
            "Epoch 159/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 3.3690 - accuracy: 0.8351\n",
            "Epoch 160/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 4.0183 - accuracy: 0.8257\n",
            "Epoch 161/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 3.0222 - accuracy: 0.8288\n",
            "Epoch 162/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 3.8178 - accuracy: 0.8430\n",
            "Epoch 163/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 3.8105 - accuracy: 0.8576\n",
            "Epoch 164/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 3.0200 - accuracy: 0.8659\n",
            "Epoch 165/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 2.8034 - accuracy: 0.8269\n",
            "Epoch 166/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 3.1061 - accuracy: 0.8501\n",
            "Epoch 167/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 3.4806 - accuracy: 0.8342\n",
            "Epoch 168/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 2.5884 - accuracy: 0.8645\n",
            "Epoch 169/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 3.4534 - accuracy: 0.8785\n",
            "Epoch 170/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 2.3710 - accuracy: 0.8607\n",
            "Epoch 171/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 2.6706 - accuracy: 0.8268\n",
            "Epoch 172/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 2.7166 - accuracy: 0.8295\n",
            "Epoch 173/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 2.6023 - accuracy: 0.8533\n",
            "Epoch 174/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 2.3038 - accuracy: 0.8548\n",
            "Epoch 175/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 2.9836 - accuracy: 0.8191\n",
            "Epoch 176/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 2.2772 - accuracy: 0.8487\n",
            "Epoch 177/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 2.4589 - accuracy: 0.8379\n",
            "Epoch 178/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 2.2326 - accuracy: 0.8297\n",
            "Epoch 179/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 2.4091 - accuracy: 0.8408\n",
            "Epoch 180/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 2.1710 - accuracy: 0.8466\n",
            "Epoch 181/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 2.5360 - accuracy: 0.8509\n",
            "Epoch 182/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 2.4007 - accuracy: 0.8395\n",
            "Epoch 183/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 2.4176 - accuracy: 0.8523\n",
            "Epoch 184/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 2.1571 - accuracy: 0.8558\n",
            "Epoch 185/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 2.1270 - accuracy: 0.8375\n",
            "Epoch 186/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 3.4542 - accuracy: 0.8259\n",
            "Epoch 187/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 2.0411 - accuracy: 0.8633\n",
            "Epoch 188/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 2.6469 - accuracy: 0.8257\n",
            "Epoch 189/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 2.4935 - accuracy: 0.8533\n",
            "Epoch 190/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.9164 - accuracy: 0.8600\n",
            "Epoch 191/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 2.3211 - accuracy: 0.8429\n",
            "Epoch 192/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.9941 - accuracy: 0.8633\n",
            "Epoch 193/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 2.6642 - accuracy: 0.8513\n",
            "Epoch 194/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.8853 - accuracy: 0.8718\n",
            "Epoch 195/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.8567 - accuracy: 0.8517\n",
            "Epoch 196/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 2.0636 - accuracy: 0.8427\n",
            "Epoch 197/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 2.7382 - accuracy: 0.8497\n",
            "Epoch 198/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.8237 - accuracy: 0.8414\n",
            "Epoch 199/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 2.2231 - accuracy: 0.8535\n",
            "Epoch 200/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 2.1184 - accuracy: 0.8501\n",
            "MSE on Training Data: 1.4873342406428869\n",
            "MSE on Test Data: 1.852718117531612\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 1.8527 - accuracy: 0.8434\n",
            "Test loss (MSE): 1.8527181148529053\n",
            "Test accuracy: 0.843406617641449\n",
            "50\n",
            "relu\n",
            "5\n",
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_10 (Dense)             (None, 50)                200       \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 50)                2550      \n",
            "_________________________________________________________________\n",
            "dense_12 (Dense)             (None, 50)                2550      \n",
            "_________________________________________________________________\n",
            "dense_13 (Dense)             (None, 50)                2550      \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 50)                2550      \n",
            "_________________________________________________________________\n",
            "module_wrapper_2 (ModuleWrap (None, 50)                0         \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 3)                 153       \n",
            "=================================================================\n",
            "Total params: 10,553\n",
            "Trainable params: 10,553\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/200\n",
            "85/85 [==============================] - 1s 3ms/step - loss: 1480.4933 - accuracy: 0.2202\n",
            "Epoch 2/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 716.4544 - accuracy: 0.2959\n",
            "Epoch 3/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 711.1657 - accuracy: 0.2832\n",
            "Epoch 4/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 703.1309 - accuracy: 0.3246\n",
            "Epoch 5/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 698.2111 - accuracy: 0.3034\n",
            "Epoch 6/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 697.7092 - accuracy: 0.2742\n",
            "Epoch 7/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 694.5006 - accuracy: 0.2916\n",
            "Epoch 8/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 688.2725 - accuracy: 0.2752\n",
            "Epoch 9/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 670.1836 - accuracy: 0.3529\n",
            "Epoch 10/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 675.1092 - accuracy: 0.3564\n",
            "Epoch 11/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 646.5184 - accuracy: 0.2914\n",
            "Epoch 12/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 613.4688 - accuracy: 0.2932\n",
            "Epoch 13/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 543.0064 - accuracy: 0.3689\n",
            "Epoch 14/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 516.6742 - accuracy: 0.3886\n",
            "Epoch 15/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 486.8247 - accuracy: 0.3811\n",
            "Epoch 16/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 486.3314 - accuracy: 0.3005\n",
            "Epoch 17/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 486.4070 - accuracy: 0.3967\n",
            "Epoch 18/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 479.8459 - accuracy: 0.3170\n",
            "Epoch 19/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 478.1355 - accuracy: 0.3145\n",
            "Epoch 20/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 466.8480 - accuracy: 0.3448\n",
            "Epoch 21/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 475.0048 - accuracy: 0.3087\n",
            "Epoch 22/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 465.2944 - accuracy: 0.3737\n",
            "Epoch 23/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 460.9247 - accuracy: 0.3823\n",
            "Epoch 24/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 445.8343 - accuracy: 0.3798\n",
            "Epoch 25/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 406.0878 - accuracy: 0.4469\n",
            "Epoch 26/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 357.0677 - accuracy: 0.4983\n",
            "Epoch 27/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 287.2095 - accuracy: 0.5978\n",
            "Epoch 28/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 284.4125 - accuracy: 0.5375\n",
            "Epoch 29/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 290.3352 - accuracy: 0.6013\n",
            "Epoch 30/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 264.5830 - accuracy: 0.5896\n",
            "Epoch 31/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 259.6527 - accuracy: 0.6013\n",
            "Epoch 32/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 267.1865 - accuracy: 0.5734\n",
            "Epoch 33/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 267.6218 - accuracy: 0.5914\n",
            "Epoch 34/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 269.4108 - accuracy: 0.6134\n",
            "Epoch 35/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 260.3719 - accuracy: 0.6374\n",
            "Epoch 36/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 244.8528 - accuracy: 0.6476\n",
            "Epoch 37/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 240.1543 - accuracy: 0.6382\n",
            "Epoch 38/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 258.7430 - accuracy: 0.6435\n",
            "Epoch 39/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 232.1040 - accuracy: 0.6739\n",
            "Epoch 40/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 245.0475 - accuracy: 0.6815\n",
            "Epoch 41/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 227.7049 - accuracy: 0.6480\n",
            "Epoch 42/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 216.6165 - accuracy: 0.6541\n",
            "Epoch 43/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 227.9164 - accuracy: 0.6759\n",
            "Epoch 44/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 214.3324 - accuracy: 0.6584\n",
            "Epoch 45/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 216.7053 - accuracy: 0.6767\n",
            "Epoch 46/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 210.3887 - accuracy: 0.6743\n",
            "Epoch 47/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 195.0360 - accuracy: 0.7052\n",
            "Epoch 48/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 186.0711 - accuracy: 0.7009\n",
            "Epoch 49/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 186.0989 - accuracy: 0.6291\n",
            "Epoch 50/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 169.7649 - accuracy: 0.6784\n",
            "Epoch 51/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 166.7065 - accuracy: 0.6908\n",
            "Epoch 52/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 150.6804 - accuracy: 0.7116\n",
            "Epoch 53/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 150.7637 - accuracy: 0.7059\n",
            "Epoch 54/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 148.5108 - accuracy: 0.7634\n",
            "Epoch 55/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 130.0040 - accuracy: 0.7349\n",
            "Epoch 56/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 138.2366 - accuracy: 0.7645\n",
            "Epoch 57/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 129.5209 - accuracy: 0.7342\n",
            "Epoch 58/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 111.0327 - accuracy: 0.7387\n",
            "Epoch 59/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 112.1647 - accuracy: 0.7443\n",
            "Epoch 60/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 126.1274 - accuracy: 0.7270\n",
            "Epoch 61/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 114.9471 - accuracy: 0.7087\n",
            "Epoch 62/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 108.9734 - accuracy: 0.7438\n",
            "Epoch 63/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 104.2746 - accuracy: 0.7581\n",
            "Epoch 64/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 107.8933 - accuracy: 0.6934\n",
            "Epoch 65/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 98.9675 - accuracy: 0.7430\n",
            "Epoch 66/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 91.9113 - accuracy: 0.7385\n",
            "Epoch 67/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 100.1269 - accuracy: 0.7383\n",
            "Epoch 68/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 84.6200 - accuracy: 0.7467\n",
            "Epoch 69/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 92.3379 - accuracy: 0.7514\n",
            "Epoch 70/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 90.0635 - accuracy: 0.7249\n",
            "Epoch 71/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 80.6761 - accuracy: 0.7209\n",
            "Epoch 72/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 83.8000 - accuracy: 0.7422\n",
            "Epoch 73/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 84.7458 - accuracy: 0.7300\n",
            "Epoch 74/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 91.2293 - accuracy: 0.7150\n",
            "Epoch 75/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 79.0217 - accuracy: 0.7458\n",
            "Epoch 76/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 86.6695 - accuracy: 0.7632\n",
            "Epoch 77/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 65.4552 - accuracy: 0.7615\n",
            "Epoch 78/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 77.9235 - accuracy: 0.7466\n",
            "Epoch 79/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 86.1059 - accuracy: 0.7574\n",
            "Epoch 80/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 77.1049 - accuracy: 0.7509\n",
            "Epoch 81/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 84.3247 - accuracy: 0.7582\n",
            "Epoch 82/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 79.8754 - accuracy: 0.7213\n",
            "Epoch 83/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 64.7866 - accuracy: 0.7478\n",
            "Epoch 84/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 70.9609 - accuracy: 0.7640\n",
            "Epoch 85/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 76.9167 - accuracy: 0.7625\n",
            "Epoch 86/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 66.8571 - accuracy: 0.7781\n",
            "Epoch 87/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 75.2915 - accuracy: 0.7406\n",
            "Epoch 88/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 69.7366 - accuracy: 0.7598\n",
            "Epoch 89/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 57.5249 - accuracy: 0.7484\n",
            "Epoch 90/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 67.6979 - accuracy: 0.7765\n",
            "Epoch 91/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 77.7958 - accuracy: 0.7290\n",
            "Epoch 92/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 58.3479 - accuracy: 0.7699\n",
            "Epoch 93/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 68.1457 - accuracy: 0.7368\n",
            "Epoch 94/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 54.0964 - accuracy: 0.7831\n",
            "Epoch 95/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 61.5335 - accuracy: 0.7499\n",
            "Epoch 96/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 71.7748 - accuracy: 0.7697\n",
            "Epoch 97/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 55.5144 - accuracy: 0.7727\n",
            "Epoch 98/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 58.8649 - accuracy: 0.7840\n",
            "Epoch 99/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 56.7828 - accuracy: 0.7477\n",
            "Epoch 100/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 63.7820 - accuracy: 0.7876\n",
            "Epoch 101/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 58.0337 - accuracy: 0.8098\n",
            "Epoch 102/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 55.3590 - accuracy: 0.7896\n",
            "Epoch 103/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 53.1558 - accuracy: 0.7735\n",
            "Epoch 104/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 54.4441 - accuracy: 0.7619\n",
            "Epoch 105/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 67.0073 - accuracy: 0.7701\n",
            "Epoch 106/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 50.3231 - accuracy: 0.7556\n",
            "Epoch 107/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 51.2456 - accuracy: 0.7481\n",
            "Epoch 108/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 47.9010 - accuracy: 0.8132\n",
            "Epoch 109/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 49.5228 - accuracy: 0.8062\n",
            "Epoch 110/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 46.7853 - accuracy: 0.8041\n",
            "Epoch 111/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 43.7806 - accuracy: 0.7836\n",
            "Epoch 112/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 48.2590 - accuracy: 0.8109\n",
            "Epoch 113/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 40.1737 - accuracy: 0.7750\n",
            "Epoch 114/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 43.1590 - accuracy: 0.7684\n",
            "Epoch 115/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 41.6871 - accuracy: 0.8104\n",
            "Epoch 116/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 34.1517 - accuracy: 0.8137\n",
            "Epoch 117/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 34.0299 - accuracy: 0.8173\n",
            "Epoch 118/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 38.1072 - accuracy: 0.7982\n",
            "Epoch 119/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 36.3330 - accuracy: 0.7918\n",
            "Epoch 120/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 33.1006 - accuracy: 0.8008\n",
            "Epoch 121/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 29.5239 - accuracy: 0.7865\n",
            "Epoch 122/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 25.5061 - accuracy: 0.7931\n",
            "Epoch 123/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 26.1640 - accuracy: 0.7907\n",
            "Epoch 124/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 24.3894 - accuracy: 0.8129\n",
            "Epoch 125/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 23.5392 - accuracy: 0.8101\n",
            "Epoch 126/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 21.7628 - accuracy: 0.8313\n",
            "Epoch 127/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 20.6258 - accuracy: 0.8215\n",
            "Epoch 128/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 19.2787 - accuracy: 0.7900\n",
            "Epoch 129/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 18.0058 - accuracy: 0.8356\n",
            "Epoch 130/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 17.4500 - accuracy: 0.8068\n",
            "Epoch 131/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 14.6603 - accuracy: 0.8086\n",
            "Epoch 132/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 15.4617 - accuracy: 0.8230\n",
            "Epoch 133/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 13.1961 - accuracy: 0.8090\n",
            "Epoch 134/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 12.5243 - accuracy: 0.8142\n",
            "Epoch 135/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 10.7718 - accuracy: 0.8140\n",
            "Epoch 136/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 10.8646 - accuracy: 0.8164\n",
            "Epoch 137/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 11.4312 - accuracy: 0.8240\n",
            "Epoch 138/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 9.2366 - accuracy: 0.8171\n",
            "Epoch 139/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 10.1967 - accuracy: 0.7983\n",
            "Epoch 140/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 9.7336 - accuracy: 0.8394\n",
            "Epoch 141/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 9.8493 - accuracy: 0.8320\n",
            "Epoch 142/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 9.2362 - accuracy: 0.8253\n",
            "Epoch 143/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 8.9673 - accuracy: 0.8080\n",
            "Epoch 144/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 7.9024 - accuracy: 0.8477\n",
            "Epoch 145/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 7.8984 - accuracy: 0.8340\n",
            "Epoch 146/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 7.8586 - accuracy: 0.8441\n",
            "Epoch 147/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 8.0640 - accuracy: 0.8327\n",
            "Epoch 148/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 7.6327 - accuracy: 0.8348\n",
            "Epoch 149/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 7.1226 - accuracy: 0.8481\n",
            "Epoch 150/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 7.0866 - accuracy: 0.8331\n",
            "Epoch 151/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 6.2999 - accuracy: 0.8215\n",
            "Epoch 152/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 6.7645 - accuracy: 0.8315\n",
            "Epoch 153/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 7.1843 - accuracy: 0.8288\n",
            "Epoch 154/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 7.9728 - accuracy: 0.8622\n",
            "Epoch 155/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 6.6166 - accuracy: 0.8232\n",
            "Epoch 156/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 9.1755 - accuracy: 0.8459\n",
            "Epoch 157/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 5.8750 - accuracy: 0.8186\n",
            "Epoch 158/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 6.2391 - accuracy: 0.8387\n",
            "Epoch 159/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 7.1399 - accuracy: 0.8489\n",
            "Epoch 160/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 5.7459 - accuracy: 0.8280\n",
            "Epoch 161/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 8.1972 - accuracy: 0.8534\n",
            "Epoch 162/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 5.0290 - accuracy: 0.8345\n",
            "Epoch 163/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 4.4080 - accuracy: 0.8589\n",
            "Epoch 164/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 5.3213 - accuracy: 0.8450\n",
            "Epoch 165/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 4.8137 - accuracy: 0.8194\n",
            "Epoch 166/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 4.6721 - accuracy: 0.8368\n",
            "Epoch 167/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 9.5002 - accuracy: 0.8412\n",
            "Epoch 168/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 4.3954 - accuracy: 0.8406\n",
            "Epoch 169/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 4.8601 - accuracy: 0.8518\n",
            "Epoch 170/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 5.3583 - accuracy: 0.8618\n",
            "Epoch 171/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 4.0609 - accuracy: 0.8388\n",
            "Epoch 172/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 3.9581 - accuracy: 0.8647\n",
            "Epoch 173/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 4.3809 - accuracy: 0.8314\n",
            "Epoch 174/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 5.3147 - accuracy: 0.8307\n",
            "Epoch 175/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 4.1587 - accuracy: 0.8356\n",
            "Epoch 176/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 4.4190 - accuracy: 0.8577\n",
            "Epoch 177/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 3.0634 - accuracy: 0.8627\n",
            "Epoch 178/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 3.8950 - accuracy: 0.8582\n",
            "Epoch 179/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 4.7648 - accuracy: 0.8598\n",
            "Epoch 180/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 4.0858 - accuracy: 0.8549\n",
            "Epoch 181/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 4.1342 - accuracy: 0.8508\n",
            "Epoch 182/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 3.6511 - accuracy: 0.8620\n",
            "Epoch 183/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 4.2177 - accuracy: 0.8439\n",
            "Epoch 184/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 3.4405 - accuracy: 0.8622\n",
            "Epoch 185/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 3.8347 - accuracy: 0.8636\n",
            "Epoch 186/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 4.3725 - accuracy: 0.8850\n",
            "Epoch 187/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 3.6843 - accuracy: 0.8534\n",
            "Epoch 188/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 4.2400 - accuracy: 0.8661\n",
            "Epoch 189/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 5.3693 - accuracy: 0.8373\n",
            "Epoch 190/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 3.1807 - accuracy: 0.8658\n",
            "Epoch 191/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 3.2500 - accuracy: 0.8553\n",
            "Epoch 192/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 3.3810 - accuracy: 0.8582\n",
            "Epoch 193/200\n",
            "85/85 [==============================] - 0s 2ms/step - loss: 2.4800 - accuracy: 0.8490\n",
            "Epoch 194/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 3.3194 - accuracy: 0.8684\n",
            "Epoch 195/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 3.2524 - accuracy: 0.8752\n",
            "Epoch 196/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 2.4114 - accuracy: 0.8509\n",
            "Epoch 197/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 2.9675 - accuracy: 0.8494\n",
            "Epoch 198/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 2.2696 - accuracy: 0.8639\n",
            "Epoch 199/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 2.8817 - accuracy: 0.8569\n",
            "Epoch 200/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 2.2802 - accuracy: 0.8535\n",
            "MSE on Training Data: 3.258628736113931\n",
            "MSE on Test Data: 3.3853041306650398\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 3.3853 - accuracy: 0.8571\n",
            "Test loss (MSE): 3.3853039741516113\n",
            "Test accuracy: 0.8571428656578064\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_16 (Dense)             (None, 50)                200       \n",
            "_________________________________________________________________\n",
            "dense_17 (Dense)             (None, 50)                2550      \n",
            "_________________________________________________________________\n",
            "dense_18 (Dense)             (None, 50)                2550      \n",
            "_________________________________________________________________\n",
            "dense_19 (Dense)             (None, 50)                2550      \n",
            "_________________________________________________________________\n",
            "dense_20 (Dense)             (None, 50)                2550      \n",
            "_________________________________________________________________\n",
            "dense_21 (Dense)             (None, 50)                2550      \n",
            "_________________________________________________________________\n",
            "module_wrapper_3 (ModuleWrap (None, 50)                0         \n",
            "_________________________________________________________________\n",
            "dense_22 (Dense)             (None, 3)                 153       \n",
            "=================================================================\n",
            "Total params: 13,103\n",
            "Trainable params: 13,103\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/200\n",
            "85/85 [==============================] - 1s 3ms/step - loss: 1420.5411 - accuracy: 0.2999\n",
            "Epoch 2/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 713.0314 - accuracy: 0.2549\n",
            "Epoch 3/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 691.3966 - accuracy: 0.2943\n",
            "Epoch 4/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 704.9116 - accuracy: 0.3008\n",
            "Epoch 5/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 690.7596 - accuracy: 0.2698\n",
            "Epoch 6/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 686.2559 - accuracy: 0.3817\n",
            "Epoch 7/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 677.0332 - accuracy: 0.4065\n",
            "Epoch 8/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 658.6321 - accuracy: 0.2597\n",
            "Epoch 9/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 632.4995 - accuracy: 0.4282\n",
            "Epoch 10/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 544.1280 - accuracy: 0.3686\n",
            "Epoch 11/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 511.4373 - accuracy: 0.4449\n",
            "Epoch 12/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 485.3826 - accuracy: 0.3412\n",
            "Epoch 13/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 479.6274 - accuracy: 0.4105\n",
            "Epoch 14/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 479.3653 - accuracy: 0.3495\n",
            "Epoch 15/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 470.4080 - accuracy: 0.2922\n",
            "Epoch 16/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 482.9489 - accuracy: 0.2675\n",
            "Epoch 17/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 491.1474 - accuracy: 0.4224\n",
            "Epoch 18/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 474.4729 - accuracy: 0.3208\n",
            "Epoch 19/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 489.2353 - accuracy: 0.3601\n",
            "Epoch 20/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 478.3698 - accuracy: 0.3274\n",
            "Epoch 21/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 482.7755 - accuracy: 0.3511\n",
            "Epoch 22/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 493.1205 - accuracy: 0.4334\n",
            "Epoch 23/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 472.5795 - accuracy: 0.3105\n",
            "Epoch 24/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 482.4649 - accuracy: 0.4248\n",
            "Epoch 25/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 479.8950 - accuracy: 0.3432\n",
            "Epoch 26/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 494.4565 - accuracy: 0.3913\n",
            "Epoch 27/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 474.1311 - accuracy: 0.3499\n",
            "Epoch 28/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 471.7131 - accuracy: 0.4093\n",
            "Epoch 29/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 488.5995 - accuracy: 0.4655\n",
            "Epoch 30/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 482.3369 - accuracy: 0.3593\n",
            "Epoch 31/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 489.6202 - accuracy: 0.4777\n",
            "Epoch 32/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 482.4156 - accuracy: 0.3460\n",
            "Epoch 33/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 493.2121 - accuracy: 0.3623\n",
            "Epoch 34/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 487.9175 - accuracy: 0.3665\n",
            "Epoch 35/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 462.5381 - accuracy: 0.3741\n",
            "Epoch 36/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 466.7578 - accuracy: 0.5684\n",
            "Epoch 37/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 388.9928 - accuracy: 0.5355\n",
            "Epoch 38/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 259.8796 - accuracy: 0.5078\n",
            "Epoch 39/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 263.4312 - accuracy: 0.5454\n",
            "Epoch 40/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 248.9974 - accuracy: 0.5166\n",
            "Epoch 41/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 251.6395 - accuracy: 0.5460\n",
            "Epoch 42/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 235.0314 - accuracy: 0.5441\n",
            "Epoch 43/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 223.0330 - accuracy: 0.5862\n",
            "Epoch 44/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 235.6879 - accuracy: 0.6117\n",
            "Epoch 45/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 215.7717 - accuracy: 0.5741\n",
            "Epoch 46/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 196.1919 - accuracy: 0.6140\n",
            "Epoch 47/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 218.9078 - accuracy: 0.6517\n",
            "Epoch 48/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 192.2553 - accuracy: 0.6296\n",
            "Epoch 49/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 198.8461 - accuracy: 0.6649\n",
            "Epoch 50/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 183.5394 - accuracy: 0.6579\n",
            "Epoch 51/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 172.4349 - accuracy: 0.6825\n",
            "Epoch 52/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 172.2949 - accuracy: 0.6740\n",
            "Epoch 53/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 188.4311 - accuracy: 0.6681\n",
            "Epoch 54/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 181.9517 - accuracy: 0.6890\n",
            "Epoch 55/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 163.0511 - accuracy: 0.7223\n",
            "Epoch 56/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 163.0596 - accuracy: 0.6830\n",
            "Epoch 57/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 150.6711 - accuracy: 0.7010\n",
            "Epoch 58/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 144.1688 - accuracy: 0.7147\n",
            "Epoch 59/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 159.9621 - accuracy: 0.7290\n",
            "Epoch 60/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 134.4005 - accuracy: 0.7462\n",
            "Epoch 61/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 117.3025 - accuracy: 0.7440\n",
            "Epoch 62/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 145.8220 - accuracy: 0.7375\n",
            "Epoch 63/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 132.7804 - accuracy: 0.7155\n",
            "Epoch 64/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 121.0219 - accuracy: 0.7280\n",
            "Epoch 65/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 127.8630 - accuracy: 0.7256\n",
            "Epoch 66/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 125.0456 - accuracy: 0.7384\n",
            "Epoch 67/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 119.9457 - accuracy: 0.7623\n",
            "Epoch 68/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 104.0602 - accuracy: 0.7652\n",
            "Epoch 69/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 96.6619 - accuracy: 0.7945\n",
            "Epoch 70/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 95.9243 - accuracy: 0.7909\n",
            "Epoch 71/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 71.7446 - accuracy: 0.7720\n",
            "Epoch 72/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 57.1495 - accuracy: 0.8088\n",
            "Epoch 73/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 35.3731 - accuracy: 0.7809\n",
            "Epoch 74/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 25.3223 - accuracy: 0.8368\n",
            "Epoch 75/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 19.8078 - accuracy: 0.8596\n",
            "Epoch 76/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 14.0881 - accuracy: 0.8370\n",
            "Epoch 77/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 12.8972 - accuracy: 0.8718\n",
            "Epoch 78/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 10.7317 - accuracy: 0.8422\n",
            "Epoch 79/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 7.7221 - accuracy: 0.8585\n",
            "Epoch 80/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 7.9767 - accuracy: 0.8587\n",
            "Epoch 81/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 6.7211 - accuracy: 0.8794\n",
            "Epoch 82/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 4.3529 - accuracy: 0.8959\n",
            "Epoch 83/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 5.2546 - accuracy: 0.8778\n",
            "Epoch 84/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 4.7674 - accuracy: 0.8853\n",
            "Epoch 85/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 3.7545 - accuracy: 0.8731\n",
            "Epoch 86/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 3.3816 - accuracy: 0.9037\n",
            "Epoch 87/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 3.8718 - accuracy: 0.8656\n",
            "Epoch 88/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 5.1758 - accuracy: 0.8674\n",
            "Epoch 89/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 3.8111 - accuracy: 0.8780\n",
            "Epoch 90/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 3.5590 - accuracy: 0.8776\n",
            "Epoch 91/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 2.9411 - accuracy: 0.8888\n",
            "Epoch 92/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 2.9638 - accuracy: 0.8957\n",
            "Epoch 93/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 4.0854 - accuracy: 0.8666\n",
            "Epoch 94/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 2.7307 - accuracy: 0.8883\n",
            "Epoch 95/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 2.6464 - accuracy: 0.8937\n",
            "Epoch 96/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 2.2716 - accuracy: 0.8938\n",
            "Epoch 97/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 3.1015 - accuracy: 0.8796\n",
            "Epoch 98/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 2.6104 - accuracy: 0.8656\n",
            "Epoch 99/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 2.0948 - accuracy: 0.8699\n",
            "Epoch 100/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 2.1569 - accuracy: 0.8626\n",
            "Epoch 101/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 2.6986 - accuracy: 0.8631\n",
            "Epoch 102/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 2.2645 - accuracy: 0.8785\n",
            "Epoch 103/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 3.4587 - accuracy: 0.8679\n",
            "Epoch 104/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.2445 - accuracy: 0.8872\n",
            "Epoch 105/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.7049 - accuracy: 0.8630\n",
            "Epoch 106/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.5376 - accuracy: 0.8570\n",
            "Epoch 107/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 2.3806 - accuracy: 0.8589\n",
            "Epoch 108/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.3960 - accuracy: 0.8789\n",
            "Epoch 109/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.4967 - accuracy: 0.8633\n",
            "Epoch 110/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.9431 - accuracy: 0.8578\n",
            "Epoch 111/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.4104 - accuracy: 0.8680\n",
            "Epoch 112/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.9336 - accuracy: 0.8844\n",
            "Epoch 113/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.5814 - accuracy: 0.8516\n",
            "Epoch 114/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 2.9077 - accuracy: 0.8566\n",
            "Epoch 115/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 3.8312 - accuracy: 0.8557\n",
            "Epoch 116/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.2985 - accuracy: 0.8789\n",
            "Epoch 117/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.4379 - accuracy: 0.8764\n",
            "Epoch 118/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 2.3594 - accuracy: 0.8436\n",
            "Epoch 119/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.4558 - accuracy: 0.8798\n",
            "Epoch 120/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.4879 - accuracy: 0.8856\n",
            "Epoch 121/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 2.2624 - accuracy: 0.8644\n",
            "Epoch 122/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.9147 - accuracy: 0.8790\n",
            "Epoch 123/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 2.0126 - accuracy: 0.8676\n",
            "Epoch 124/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.5426 - accuracy: 0.8924\n",
            "Epoch 125/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 3.0271 - accuracy: 0.8664\n",
            "Epoch 126/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 2.3760 - accuracy: 0.8643\n",
            "Epoch 127/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 2.9658 - accuracy: 0.8727\n",
            "Epoch 128/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.5481 - accuracy: 0.8751\n",
            "Epoch 129/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.3848 - accuracy: 0.8642\n",
            "Epoch 130/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.0698 - accuracy: 0.8828\n",
            "Epoch 131/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.8357 - accuracy: 0.8620\n",
            "Epoch 132/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.5056 - accuracy: 0.8820\n",
            "Epoch 133/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.0420 - accuracy: 0.8685\n",
            "Epoch 134/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.8706 - accuracy: 0.8811\n",
            "Epoch 135/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.8410 - accuracy: 0.8741\n",
            "Epoch 136/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.1675 - accuracy: 0.8758\n",
            "Epoch 137/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 2.0883 - accuracy: 0.8682\n",
            "Epoch 138/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.1640 - accuracy: 0.8769\n",
            "Epoch 139/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.2014 - accuracy: 0.8957\n",
            "Epoch 140/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.3074 - accuracy: 0.8766\n",
            "Epoch 141/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 3.1999 - accuracy: 0.8657\n",
            "Epoch 142/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.2381 - accuracy: 0.8755\n",
            "Epoch 143/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.8702 - accuracy: 0.8917\n",
            "Epoch 144/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.2553 - accuracy: 0.8888\n",
            "Epoch 145/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.5387 - accuracy: 0.8701\n",
            "Epoch 146/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.8715 - accuracy: 0.8678\n",
            "Epoch 147/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.1971 - accuracy: 0.8527\n",
            "Epoch 148/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.2054 - accuracy: 0.8545\n",
            "Epoch 149/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 2.5607 - accuracy: 0.8603\n",
            "Epoch 150/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.6254 - accuracy: 0.8966\n",
            "Epoch 151/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 3.4748 - accuracy: 0.8717\n",
            "Epoch 152/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.9425 - accuracy: 0.8639\n",
            "Epoch 153/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.2205 - accuracy: 0.8626\n",
            "Epoch 154/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.2492 - accuracy: 0.8730\n",
            "Epoch 155/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 3.5600 - accuracy: 0.8677\n",
            "Epoch 156/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 2.5937 - accuracy: 0.8749\n",
            "Epoch 157/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.8531 - accuracy: 0.8682\n",
            "Epoch 158/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.2529 - accuracy: 0.8606\n",
            "Epoch 159/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.8811 - accuracy: 0.8350\n",
            "Epoch 160/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.1185 - accuracy: 0.8460\n",
            "Epoch 161/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.0101 - accuracy: 0.8662\n",
            "Epoch 162/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.8395 - accuracy: 0.8839\n",
            "Epoch 163/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.9902 - accuracy: 0.8856\n",
            "Epoch 164/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.7373 - accuracy: 0.8657\n",
            "Epoch 165/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 4.7477 - accuracy: 0.8852\n",
            "Epoch 166/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 2.1420 - accuracy: 0.8556\n",
            "Epoch 167/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.9407 - accuracy: 0.8894\n",
            "Epoch 168/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.6725 - accuracy: 0.8820\n",
            "Epoch 169/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.6915 - accuracy: 0.8530\n",
            "Epoch 170/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 2.4878 - accuracy: 0.8402\n",
            "Epoch 171/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.4503 - accuracy: 0.8805\n",
            "Epoch 172/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.9530 - accuracy: 0.8781\n",
            "Epoch 173/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.0330 - accuracy: 0.8977\n",
            "Epoch 174/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.5443 - accuracy: 0.8972\n",
            "Epoch 175/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.7850 - accuracy: 0.8724\n",
            "Epoch 176/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.8016 - accuracy: 0.8417\n",
            "Epoch 177/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 2.1883 - accuracy: 0.8589\n",
            "Epoch 178/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.9933 - accuracy: 0.8982\n",
            "Epoch 179/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.0337 - accuracy: 0.8719\n",
            "Epoch 180/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.3508 - accuracy: 0.8608\n",
            "Epoch 181/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 9.7028 - accuracy: 0.8333\n",
            "Epoch 182/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.2932 - accuracy: 0.8707\n",
            "Epoch 183/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 2.9147 - accuracy: 0.8417\n",
            "Epoch 184/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.3029 - accuracy: 0.8802\n",
            "Epoch 185/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.8590 - accuracy: 0.8568\n",
            "Epoch 186/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.5249 - accuracy: 0.8660\n",
            "Epoch 187/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.5092 - accuracy: 0.8687\n",
            "Epoch 188/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.7528 - accuracy: 0.8698\n",
            "Epoch 189/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.5975 - accuracy: 0.8686\n",
            "Epoch 190/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.8633 - accuracy: 0.8805\n",
            "Epoch 191/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.9655 - accuracy: 0.8633\n",
            "Epoch 192/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.8133 - accuracy: 0.8669\n",
            "Epoch 193/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.9930 - accuracy: 0.8467\n",
            "Epoch 194/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.1870 - accuracy: 0.8662\n",
            "Epoch 195/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.0130 - accuracy: 0.8793\n",
            "Epoch 196/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.6226 - accuracy: 0.8887\n",
            "Epoch 197/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.7268 - accuracy: 0.8829\n",
            "Epoch 198/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.6296 - accuracy: 0.8590\n",
            "Epoch 199/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.9230 - accuracy: 0.8809\n",
            "Epoch 200/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.0516 - accuracy: 0.8614\n",
            "MSE on Training Data: 0.25020186461026617\n",
            "MSE on Test Data: 0.2717792138691399\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.2718 - accuracy: 0.8571\n",
            "Test loss (MSE): 0.27177920937538147\n",
            "Test accuracy: 0.8571428656578064\n",
            "50\n",
            "relu\n",
            "7\n",
            "Model: \"sequential_4\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_23 (Dense)             (None, 50)                200       \n",
            "_________________________________________________________________\n",
            "dense_24 (Dense)             (None, 50)                2550      \n",
            "_________________________________________________________________\n",
            "dense_25 (Dense)             (None, 50)                2550      \n",
            "_________________________________________________________________\n",
            "dense_26 (Dense)             (None, 50)                2550      \n",
            "_________________________________________________________________\n",
            "dense_27 (Dense)             (None, 50)                2550      \n",
            "_________________________________________________________________\n",
            "dense_28 (Dense)             (None, 50)                2550      \n",
            "_________________________________________________________________\n",
            "dense_29 (Dense)             (None, 50)                2550      \n",
            "_________________________________________________________________\n",
            "dense_30 (Dense)             (None, 50)                2550      \n",
            "_________________________________________________________________\n",
            "module_wrapper_4 (ModuleWrap (None, 50)                0         \n",
            "_________________________________________________________________\n",
            "dense_31 (Dense)             (None, 3)                 153       \n",
            "=================================================================\n",
            "Total params: 18,203\n",
            "Trainable params: 18,203\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/200\n",
            "85/85 [==============================] - 1s 3ms/step - loss: 1375.4460 - accuracy: 0.3026\n",
            "Epoch 2/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 710.6165 - accuracy: 0.3289\n",
            "Epoch 3/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 714.1006 - accuracy: 0.3486\n",
            "Epoch 4/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 704.8113 - accuracy: 0.3946\n",
            "Epoch 5/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 702.8626 - accuracy: 0.2897\n",
            "Epoch 6/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 697.8188 - accuracy: 0.3248\n",
            "Epoch 7/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 688.3657 - accuracy: 0.4212\n",
            "Epoch 8/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 675.3087 - accuracy: 0.2886\n",
            "Epoch 9/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 618.8226 - accuracy: 0.2641\n",
            "Epoch 10/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 536.5864 - accuracy: 0.3125\n",
            "Epoch 11/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 515.6662 - accuracy: 0.3613\n",
            "Epoch 12/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 504.6369 - accuracy: 0.3912\n",
            "Epoch 13/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 505.5330 - accuracy: 0.3577\n",
            "Epoch 14/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 488.4003 - accuracy: 0.3622\n",
            "Epoch 15/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 511.1554 - accuracy: 0.4026\n",
            "Epoch 16/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 491.8297 - accuracy: 0.4576\n",
            "Epoch 17/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 486.6332 - accuracy: 0.4559\n",
            "Epoch 18/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 494.8400 - accuracy: 0.3542\n",
            "Epoch 19/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 472.1258 - accuracy: 0.3616\n",
            "Epoch 20/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 486.6561 - accuracy: 0.3881\n",
            "Epoch 21/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 493.6758 - accuracy: 0.4002\n",
            "Epoch 22/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 492.5237 - accuracy: 0.4473\n",
            "Epoch 23/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 487.1974 - accuracy: 0.3535\n",
            "Epoch 24/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 494.3054 - accuracy: 0.3940\n",
            "Epoch 25/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 470.9403 - accuracy: 0.3649\n",
            "Epoch 26/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 489.6919 - accuracy: 0.4878\n",
            "Epoch 27/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 485.5685 - accuracy: 0.4640\n",
            "Epoch 28/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 463.3230 - accuracy: 0.4430\n",
            "Epoch 29/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 475.2449 - accuracy: 0.4396\n",
            "Epoch 30/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 455.3354 - accuracy: 0.5040\n",
            "Epoch 31/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 429.7558 - accuracy: 0.5385\n",
            "Epoch 32/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 320.3615 - accuracy: 0.5434\n",
            "Epoch 33/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 241.9241 - accuracy: 0.5969\n",
            "Epoch 34/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 234.2707 - accuracy: 0.6303\n",
            "Epoch 35/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 220.2817 - accuracy: 0.6516\n",
            "Epoch 36/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 204.8537 - accuracy: 0.6622\n",
            "Epoch 37/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 187.5566 - accuracy: 0.7439\n",
            "Epoch 38/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 183.4450 - accuracy: 0.7200\n",
            "Epoch 39/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 202.7986 - accuracy: 0.6778\n",
            "Epoch 40/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 162.4540 - accuracy: 0.7313\n",
            "Epoch 41/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 166.4734 - accuracy: 0.7078\n",
            "Epoch 42/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 131.5651 - accuracy: 0.6894\n",
            "Epoch 43/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 171.9558 - accuracy: 0.6449\n",
            "Epoch 44/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 146.1537 - accuracy: 0.6873\n",
            "Epoch 45/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 145.2861 - accuracy: 0.7186\n",
            "Epoch 46/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 123.2744 - accuracy: 0.7393\n",
            "Epoch 47/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 129.3900 - accuracy: 0.7150\n",
            "Epoch 48/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 148.3299 - accuracy: 0.7212\n",
            "Epoch 49/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 118.4686 - accuracy: 0.7529\n",
            "Epoch 50/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 113.1674 - accuracy: 0.7603\n",
            "Epoch 51/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 103.1854 - accuracy: 0.7812\n",
            "Epoch 52/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 124.1601 - accuracy: 0.7459\n",
            "Epoch 53/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 103.9972 - accuracy: 0.7604\n",
            "Epoch 54/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 90.3202 - accuracy: 0.7857\n",
            "Epoch 55/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 74.6737 - accuracy: 0.7743\n",
            "Epoch 56/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 29.7518 - accuracy: 0.8432\n",
            "Epoch 57/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 19.2879 - accuracy: 0.8750\n",
            "Epoch 58/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 10.4261 - accuracy: 0.8920\n",
            "Epoch 59/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 9.2544 - accuracy: 0.8637\n",
            "Epoch 60/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 6.0205 - accuracy: 0.8791\n",
            "Epoch 61/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 6.5212 - accuracy: 0.8581\n",
            "Epoch 62/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 8.2255 - accuracy: 0.8592\n",
            "Epoch 63/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 6.2253 - accuracy: 0.8921\n",
            "Epoch 64/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 4.3723 - accuracy: 0.8948\n",
            "Epoch 65/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 4.3942 - accuracy: 0.9020\n",
            "Epoch 66/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 3.5880 - accuracy: 0.8978\n",
            "Epoch 67/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 3.4380 - accuracy: 0.8879\n",
            "Epoch 68/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 2.9336 - accuracy: 0.8997\n",
            "Epoch 69/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 3.4650 - accuracy: 0.9216\n",
            "Epoch 70/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 2.8780 - accuracy: 0.8986\n",
            "Epoch 71/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 4.5819 - accuracy: 0.8803\n",
            "Epoch 72/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 2.9690 - accuracy: 0.8970\n",
            "Epoch 73/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 7.7411 - accuracy: 0.8572\n",
            "Epoch 74/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 4.8886 - accuracy: 0.8706\n",
            "Epoch 75/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 2.6068 - accuracy: 0.8981\n",
            "Epoch 76/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 3.1573 - accuracy: 0.8843\n",
            "Epoch 77/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 4.2856 - accuracy: 0.8585\n",
            "Epoch 78/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 3.3277 - accuracy: 0.9037\n",
            "Epoch 79/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 2.2708 - accuracy: 0.8888\n",
            "Epoch 80/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.8450 - accuracy: 0.9058\n",
            "Epoch 81/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.7833 - accuracy: 0.9112\n",
            "Epoch 82/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 3.9461 - accuracy: 0.8709\n",
            "Epoch 83/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 2.2577 - accuracy: 0.8788\n",
            "Epoch 84/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 5.2438 - accuracy: 0.8773\n",
            "Epoch 85/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 3.1504 - accuracy: 0.8600\n",
            "Epoch 86/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 2.2048 - accuracy: 0.8696\n",
            "Epoch 87/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.8154 - accuracy: 0.8916\n",
            "Epoch 88/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.5399 - accuracy: 0.8934\n",
            "Epoch 89/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.5718 - accuracy: 0.8833\n",
            "Epoch 90/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 3.8921 - accuracy: 0.8743\n",
            "Epoch 91/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 5.0246 - accuracy: 0.8729\n",
            "Epoch 92/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 4.9171 - accuracy: 0.8599\n",
            "Epoch 93/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.3080 - accuracy: 0.8983\n",
            "Epoch 94/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.8196 - accuracy: 0.8786\n",
            "Epoch 95/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.6969 - accuracy: 0.8452\n",
            "Epoch 96/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.5831 - accuracy: 0.8750\n",
            "Epoch 97/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.5182 - accuracy: 0.8728\n",
            "Epoch 98/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 2.2585 - accuracy: 0.8700\n",
            "Epoch 99/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 2.3667 - accuracy: 0.9020\n",
            "Epoch 100/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.9674 - accuracy: 0.8822\n",
            "Epoch 101/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 2.7116 - accuracy: 0.8872\n",
            "Epoch 102/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 2.0683 - accuracy: 0.8880\n",
            "Epoch 103/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 3.9221 - accuracy: 0.8714\n",
            "Epoch 104/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 2.9416 - accuracy: 0.8843\n",
            "Epoch 105/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 2.4916 - accuracy: 0.8726\n",
            "Epoch 106/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 1.4402 - accuracy: 0.8913\n",
            "Epoch 107/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 8.4393 - accuracy: 0.8395\n",
            "Epoch 108/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 2.4469 - accuracy: 0.8763\n",
            "Epoch 109/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 10.0353 - accuracy: 0.8357\n",
            "Epoch 110/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 4.6000 - accuracy: 0.8791\n",
            "Epoch 111/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 4.4133 - accuracy: 0.8263\n",
            "Epoch 112/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 7.0924 - accuracy: 0.8402\n",
            "Epoch 113/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.8482 - accuracy: 0.8368\n",
            "Epoch 114/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 1.6410 - accuracy: 0.8671\n",
            "Epoch 115/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.9717 - accuracy: 0.8606\n",
            "Epoch 116/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.0071 - accuracy: 0.8884\n",
            "Epoch 117/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 2.4683 - accuracy: 0.8547\n",
            "Epoch 118/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 2.5157 - accuracy: 0.8797\n",
            "Epoch 119/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.9817 - accuracy: 0.9110\n",
            "Epoch 120/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.1189 - accuracy: 0.8768\n",
            "Epoch 121/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.6661 - accuracy: 0.8931\n",
            "Epoch 122/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 5.3626 - accuracy: 0.8589\n",
            "Epoch 123/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 2.0056 - accuracy: 0.8699\n",
            "Epoch 124/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.2151 - accuracy: 0.8784\n",
            "Epoch 125/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.7667 - accuracy: 0.8807\n",
            "Epoch 126/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.3171 - accuracy: 0.8871\n",
            "Epoch 127/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 17.7877 - accuracy: 0.8326\n",
            "Epoch 128/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.5602 - accuracy: 0.8646\n",
            "Epoch 129/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 2.5667 - accuracy: 0.8445\n",
            "Epoch 130/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.3752 - accuracy: 0.8674\n",
            "Epoch 131/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 2.2770 - accuracy: 0.8618\n",
            "Epoch 132/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.8625 - accuracy: 0.8967\n",
            "Epoch 133/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.6744 - accuracy: 0.8775\n",
            "Epoch 134/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.6631 - accuracy: 0.8630\n",
            "Epoch 135/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 5.2974 - accuracy: 0.8589\n",
            "Epoch 136/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.4173 - accuracy: 0.8630\n",
            "Epoch 137/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.6565 - accuracy: 0.8751\n",
            "Epoch 138/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.2869 - accuracy: 0.8740\n",
            "Epoch 139/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 5.2781 - accuracy: 0.8516\n",
            "Epoch 140/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 3.1496 - accuracy: 0.8304\n",
            "Epoch 141/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 11.8224 - accuracy: 0.8381\n",
            "Epoch 142/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 8.2468 - accuracy: 0.8316\n",
            "Epoch 143/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 3.7994 - accuracy: 0.8649\n",
            "Epoch 144/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.2811 - accuracy: 0.8740\n",
            "Epoch 145/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 2.2321 - accuracy: 0.8766\n",
            "Epoch 146/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.7962 - accuracy: 0.8510\n",
            "Epoch 147/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 3.2284 - accuracy: 0.8634\n",
            "Epoch 148/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.1153 - accuracy: 0.8850\n",
            "Epoch 149/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.7032 - accuracy: 0.8519\n",
            "Epoch 150/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.2414 - accuracy: 0.8501\n",
            "Epoch 151/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.4862 - accuracy: 0.8832\n",
            "Epoch 152/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.9818 - accuracy: 0.8618\n",
            "Epoch 153/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.8750 - accuracy: 0.8635\n",
            "Epoch 154/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 3.4865 - accuracy: 0.8450\n",
            "Epoch 155/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 6.4559 - accuracy: 0.8257\n",
            "Epoch 156/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 11.1989 - accuracy: 0.8176\n",
            "Epoch 157/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.7995 - accuracy: 0.8713\n",
            "Epoch 158/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.9592 - accuracy: 0.8815\n",
            "Epoch 159/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.0858 - accuracy: 0.8481\n",
            "Epoch 160/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.5498 - accuracy: 0.8715\n",
            "Epoch 161/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 1.1055 - accuracy: 0.8946\n",
            "Epoch 162/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.5667 - accuracy: 0.8704\n",
            "Epoch 163/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.5539 - accuracy: 0.8915\n",
            "Epoch 164/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.0092 - accuracy: 0.8794\n",
            "Epoch 165/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.7502 - accuracy: 0.8739\n",
            "Epoch 166/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.6903 - accuracy: 0.8595\n",
            "Epoch 167/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 3.0440 - accuracy: 0.8772\n",
            "Epoch 168/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.6623 - accuracy: 0.8516\n",
            "Epoch 169/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 2.5659 - accuracy: 0.8635\n",
            "Epoch 170/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 7.2082 - accuracy: 0.8365\n",
            "Epoch 171/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 2.5291 - accuracy: 0.8741\n",
            "Epoch 172/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 2.4042 - accuracy: 0.8599\n",
            "Epoch 173/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.7898 - accuracy: 0.8782\n",
            "Epoch 174/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.7461 - accuracy: 0.8448\n",
            "Epoch 175/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.6503 - accuracy: 0.8562\n",
            "Epoch 176/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 2.6022 - accuracy: 0.8399\n",
            "Epoch 177/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.8420 - accuracy: 0.8673\n",
            "Epoch 178/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.1473 - accuracy: 0.8408\n",
            "Epoch 179/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.7421 - accuracy: 0.8380\n",
            "Epoch 180/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.8303 - accuracy: 0.8601\n",
            "Epoch 181/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 2.3378 - accuracy: 0.8293\n",
            "Epoch 182/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.9359 - accuracy: 0.8852\n",
            "Epoch 183/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.8976 - accuracy: 0.8647\n",
            "Epoch 184/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 3.0689 - accuracy: 0.8489\n",
            "Epoch 185/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 2.9107 - accuracy: 0.8467\n",
            "Epoch 186/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.9392 - accuracy: 0.8500\n",
            "Epoch 187/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.3496 - accuracy: 0.8708\n",
            "Epoch 188/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 6.5175 - accuracy: 0.7940\n",
            "Epoch 189/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 7.8088 - accuracy: 0.8115\n",
            "Epoch 190/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.5168 - accuracy: 0.8904\n",
            "Epoch 191/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.4104 - accuracy: 0.8673\n",
            "Epoch 192/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.8135 - accuracy: 0.8686\n",
            "Epoch 193/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.3878 - accuracy: 0.8550\n",
            "Epoch 194/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.4293 - accuracy: 0.8974\n",
            "Epoch 195/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.7811 - accuracy: 0.8763\n",
            "Epoch 196/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.7512 - accuracy: 0.8778\n",
            "Epoch 197/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 5.2176 - accuracy: 0.8510\n",
            "Epoch 198/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.7234 - accuracy: 0.8522\n",
            "Epoch 199/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.7466 - accuracy: 0.8553\n",
            "Epoch 200/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.4700 - accuracy: 0.8725\n",
            "MSE on Training Data: 0.19401836706978634\n",
            "MSE on Test Data: 0.2162743701769497\n",
            "12/12 [==============================] - 0s 2ms/step - loss: 0.2163 - accuracy: 0.9478\n",
            "Test loss (MSE): 0.21627436578273773\n",
            "Test accuracy: 0.9478021860122681\n",
            "50\n",
            "relu\n",
            "9\n",
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_32 (Dense)             (None, 50)                200       \n",
            "_________________________________________________________________\n",
            "dense_33 (Dense)             (None, 50)                2550      \n",
            "_________________________________________________________________\n",
            "dense_34 (Dense)             (None, 50)                2550      \n",
            "_________________________________________________________________\n",
            "dense_35 (Dense)             (None, 50)                2550      \n",
            "_________________________________________________________________\n",
            "dense_36 (Dense)             (None, 50)                2550      \n",
            "_________________________________________________________________\n",
            "dense_37 (Dense)             (None, 50)                2550      \n",
            "_________________________________________________________________\n",
            "dense_38 (Dense)             (None, 50)                2550      \n",
            "_________________________________________________________________\n",
            "module_wrapper_5 (ModuleWrap (None, 50)                0         \n",
            "_________________________________________________________________\n",
            "dense_39 (Dense)             (None, 3)                 153       \n",
            "=================================================================\n",
            "Total params: 15,653\n",
            "Trainable params: 15,653\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/200\n",
            "85/85 [==============================] - 1s 3ms/step - loss: 1369.9310 - accuracy: 0.3190\n",
            "Epoch 2/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 705.6770 - accuracy: 0.3242\n",
            "Epoch 3/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 713.0993 - accuracy: 0.3965\n",
            "Epoch 4/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 709.7893 - accuracy: 0.3740\n",
            "Epoch 5/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 707.4674 - accuracy: 0.2605\n",
            "Epoch 6/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 686.8301 - accuracy: 0.2583\n",
            "Epoch 7/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 682.3941 - accuracy: 0.2417\n",
            "Epoch 8/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 665.3893 - accuracy: 0.2417\n",
            "Epoch 9/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 622.0719 - accuracy: 0.3140\n",
            "Epoch 10/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 518.9512 - accuracy: 0.2721\n",
            "Epoch 11/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 505.8164 - accuracy: 0.2943\n",
            "Epoch 12/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 504.5679 - accuracy: 0.2990\n",
            "Epoch 13/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 482.6350 - accuracy: 0.3146\n",
            "Epoch 14/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 491.0697 - accuracy: 0.3272\n",
            "Epoch 15/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 489.8568 - accuracy: 0.2784\n",
            "Epoch 16/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 487.9071 - accuracy: 0.2991\n",
            "Epoch 17/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 504.5389 - accuracy: 0.2957\n",
            "Epoch 18/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 481.9279 - accuracy: 0.2941\n",
            "Epoch 19/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 485.1427 - accuracy: 0.2822\n",
            "Epoch 20/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 503.3810 - accuracy: 0.2733\n",
            "Epoch 21/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 487.0460 - accuracy: 0.3975\n",
            "Epoch 22/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 480.5323 - accuracy: 0.2930\n",
            "Epoch 23/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 482.1060 - accuracy: 0.3053\n",
            "Epoch 24/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 494.8650 - accuracy: 0.4394\n",
            "Epoch 25/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 495.7726 - accuracy: 0.3355\n",
            "Epoch 26/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 434.5880 - accuracy: 0.3663\n",
            "Epoch 27/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 272.1056 - accuracy: 0.5377\n",
            "Epoch 28/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 240.9216 - accuracy: 0.6077\n",
            "Epoch 29/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 235.8785 - accuracy: 0.6233\n",
            "Epoch 30/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 236.3491 - accuracy: 0.5536\n",
            "Epoch 31/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 254.1557 - accuracy: 0.5269\n",
            "Epoch 32/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 253.6667 - accuracy: 0.5558\n",
            "Epoch 33/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 246.4741 - accuracy: 0.6276\n",
            "Epoch 34/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 233.8582 - accuracy: 0.7016\n",
            "Epoch 35/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 218.2533 - accuracy: 0.7253\n",
            "Epoch 36/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 207.7716 - accuracy: 0.7585\n",
            "Epoch 37/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 212.6356 - accuracy: 0.7466\n",
            "Epoch 38/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 214.2778 - accuracy: 0.7267\n",
            "Epoch 39/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 189.5500 - accuracy: 0.7901\n",
            "Epoch 40/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 198.9847 - accuracy: 0.7433\n",
            "Epoch 41/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 202.0176 - accuracy: 0.7563\n",
            "Epoch 42/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 190.5793 - accuracy: 0.7810\n",
            "Epoch 43/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 195.9839 - accuracy: 0.7684\n",
            "Epoch 44/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 191.3836 - accuracy: 0.7820\n",
            "Epoch 45/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 190.6389 - accuracy: 0.7841\n",
            "Epoch 46/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 173.4930 - accuracy: 0.7722\n",
            "Epoch 47/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 176.2783 - accuracy: 0.7954\n",
            "Epoch 48/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 173.6488 - accuracy: 0.7973\n",
            "Epoch 49/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 181.3247 - accuracy: 0.7454\n",
            "Epoch 50/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 163.9209 - accuracy: 0.7975\n",
            "Epoch 51/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 203.1588 - accuracy: 0.7097\n",
            "Epoch 52/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 172.3929 - accuracy: 0.7429\n",
            "Epoch 53/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 164.0982 - accuracy: 0.7843\n",
            "Epoch 54/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 183.5081 - accuracy: 0.7709\n",
            "Epoch 55/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 181.7062 - accuracy: 0.7153\n",
            "Epoch 56/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 184.3994 - accuracy: 0.7170\n",
            "Epoch 57/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 175.3933 - accuracy: 0.7091\n",
            "Epoch 58/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 174.5234 - accuracy: 0.7473\n",
            "Epoch 59/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 134.9344 - accuracy: 0.7053\n",
            "Epoch 60/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 121.8984 - accuracy: 0.7416\n",
            "Epoch 61/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 120.1633 - accuracy: 0.7393\n",
            "Epoch 62/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 118.8527 - accuracy: 0.7464\n",
            "Epoch 63/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 103.2807 - accuracy: 0.7386\n",
            "Epoch 64/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 79.3182 - accuracy: 0.7958\n",
            "Epoch 65/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 49.2942 - accuracy: 0.8267\n",
            "Epoch 66/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 22.9742 - accuracy: 0.8037\n",
            "Epoch 67/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 14.9401 - accuracy: 0.8430\n",
            "Epoch 68/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 9.1516 - accuracy: 0.8419\n",
            "Epoch 69/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 7.1667 - accuracy: 0.8758\n",
            "Epoch 70/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 4.2905 - accuracy: 0.8674\n",
            "Epoch 71/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 4.6470 - accuracy: 0.8763\n",
            "Epoch 72/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 3.8367 - accuracy: 0.8592\n",
            "Epoch 73/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 4.0095 - accuracy: 0.8743\n",
            "Epoch 74/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 3.6282 - accuracy: 0.8689\n",
            "Epoch 75/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 3.7339 - accuracy: 0.8814\n",
            "Epoch 76/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 3.6055 - accuracy: 0.8795\n",
            "Epoch 77/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 2.9963 - accuracy: 0.8694\n",
            "Epoch 78/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 3.1341 - accuracy: 0.8887\n",
            "Epoch 79/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 3.1438 - accuracy: 0.8859\n",
            "Epoch 80/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 3.7105 - accuracy: 0.8989\n",
            "Epoch 81/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 3.6539 - accuracy: 0.8838\n",
            "Epoch 82/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 3.6646 - accuracy: 0.8469\n",
            "Epoch 83/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.9865 - accuracy: 0.8916\n",
            "Epoch 84/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 2.1937 - accuracy: 0.9075\n",
            "Epoch 85/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 2.6380 - accuracy: 0.8598\n",
            "Epoch 86/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 5.4779 - accuracy: 0.8588\n",
            "Epoch 87/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 5.2222 - accuracy: 0.8503\n",
            "Epoch 88/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 2.2410 - accuracy: 0.8954\n",
            "Epoch 89/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.6055 - accuracy: 0.8829\n",
            "Epoch 90/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 2.1374 - accuracy: 0.8891\n",
            "Epoch 91/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 4.2468 - accuracy: 0.8682\n",
            "Epoch 92/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.9737 - accuracy: 0.9074\n",
            "Epoch 93/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 3.4674 - accuracy: 0.8852\n",
            "Epoch 94/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 2.4639 - accuracy: 0.8665\n",
            "Epoch 95/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.7473 - accuracy: 0.8755\n",
            "Epoch 96/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 3.8863 - accuracy: 0.8740\n",
            "Epoch 97/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 2.1673 - accuracy: 0.8885\n",
            "Epoch 98/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.6511 - accuracy: 0.8852\n",
            "Epoch 99/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 6.7152 - accuracy: 0.8587\n",
            "Epoch 100/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.7107 - accuracy: 0.8953\n",
            "Epoch 101/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.1874 - accuracy: 0.8975\n",
            "Epoch 102/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 2.3118 - accuracy: 0.9021\n",
            "Epoch 103/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.6621 - accuracy: 0.8791\n",
            "Epoch 104/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.7544 - accuracy: 0.8770\n",
            "Epoch 105/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 3.2184 - accuracy: 0.8664\n",
            "Epoch 106/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 6.2797 - accuracy: 0.8105\n",
            "Epoch 107/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 2.9741 - accuracy: 0.8719\n",
            "Epoch 108/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.6135 - accuracy: 0.8764\n",
            "Epoch 109/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.6767 - accuracy: 0.8858\n",
            "Epoch 110/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.2106 - accuracy: 0.9131\n",
            "Epoch 111/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.6151 - accuracy: 0.8669\n",
            "Epoch 112/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 3.4968 - accuracy: 0.8370\n",
            "Epoch 113/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.7743 - accuracy: 0.8987\n",
            "Epoch 114/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.6308 - accuracy: 0.8696\n",
            "Epoch 115/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 3.0130 - accuracy: 0.8601\n",
            "Epoch 116/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 3.6182 - accuracy: 0.8505\n",
            "Epoch 117/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.4206 - accuracy: 0.8667\n",
            "Epoch 118/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 2.0412 - accuracy: 0.8677\n",
            "Epoch 119/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 2.4719 - accuracy: 0.8865\n",
            "Epoch 120/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 2.2961 - accuracy: 0.8895\n",
            "Epoch 121/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 3.9698 - accuracy: 0.8651\n",
            "Epoch 122/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 4.0534 - accuracy: 0.8762\n",
            "Epoch 123/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 2.4778 - accuracy: 0.8448\n",
            "Epoch 124/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.2299 - accuracy: 0.8869\n",
            "Epoch 125/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.3313 - accuracy: 0.8968\n",
            "Epoch 126/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 2.1799 - accuracy: 0.8618\n",
            "Epoch 127/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 5.3662 - accuracy: 0.8658\n",
            "Epoch 128/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.7107 - accuracy: 0.8910\n",
            "Epoch 129/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 2.4202 - accuracy: 0.8616\n",
            "Epoch 130/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 2.6746 - accuracy: 0.8429\n",
            "Epoch 131/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.6189 - accuracy: 0.8414\n",
            "Epoch 132/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 3.0847 - accuracy: 0.8668\n",
            "Epoch 133/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.5215 - accuracy: 0.8937\n",
            "Epoch 134/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.7733 - accuracy: 0.8721\n",
            "Epoch 135/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.2306 - accuracy: 0.8688\n",
            "Epoch 136/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 4.7299 - accuracy: 0.8468\n",
            "Epoch 137/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.8873 - accuracy: 0.8749\n",
            "Epoch 138/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 1.1870 - accuracy: 0.8816\n",
            "Epoch 139/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.9553 - accuracy: 0.8780\n",
            "Epoch 140/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 1.9783 - accuracy: 0.8662\n",
            "Epoch 141/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 5.3411 - accuracy: 0.8341\n",
            "Epoch 142/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.8194 - accuracy: 0.8954\n",
            "Epoch 143/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.7826 - accuracy: 0.8634\n",
            "Epoch 144/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.1603 - accuracy: 0.8774\n",
            "Epoch 145/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 3.0345 - accuracy: 0.8657\n",
            "Epoch 146/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 1.1438 - accuracy: 0.8956\n",
            "Epoch 147/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.6393 - accuracy: 0.8738\n",
            "Epoch 148/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.2272 - accuracy: 0.8836\n",
            "Epoch 149/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 4.1175 - accuracy: 0.8597\n",
            "Epoch 150/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.9058 - accuracy: 0.8628\n",
            "Epoch 151/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 2.8033 - accuracy: 0.8490\n",
            "Epoch 152/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.9034 - accuracy: 0.8621\n",
            "Epoch 153/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 4.8569 - accuracy: 0.8340\n",
            "Epoch 154/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.3991 - accuracy: 0.8471\n",
            "Epoch 155/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.7659 - accuracy: 0.8878\n",
            "Epoch 156/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 4.9062 - accuracy: 0.8666\n",
            "Epoch 157/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 1.8349 - accuracy: 0.8662\n",
            "Epoch 158/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 3.4821 - accuracy: 0.8406\n",
            "Epoch 159/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 3.8927 - accuracy: 0.8404\n",
            "Epoch 160/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.9493 - accuracy: 0.8961\n",
            "Epoch 161/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.8303 - accuracy: 0.8535\n",
            "Epoch 162/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.2754 - accuracy: 0.8702\n",
            "Epoch 163/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 1.0559 - accuracy: 0.8790\n",
            "Epoch 164/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.7929 - accuracy: 0.8743\n",
            "Epoch 165/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 2.1116 - accuracy: 0.8592\n",
            "Epoch 166/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.2064 - accuracy: 0.8518\n",
            "Epoch 167/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.9747 - accuracy: 0.8851\n",
            "Epoch 168/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.3707 - accuracy: 0.8714\n",
            "Epoch 169/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.5483 - accuracy: 0.8748\n",
            "Epoch 170/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 1.0194 - accuracy: 0.8450\n",
            "Epoch 171/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.2067 - accuracy: 0.8734\n",
            "Epoch 172/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.5204 - accuracy: 0.8516\n",
            "Epoch 173/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.9417 - accuracy: 0.8642\n",
            "Epoch 174/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.2938 - accuracy: 0.8436\n",
            "Epoch 175/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.5877 - accuracy: 0.8654\n",
            "Epoch 176/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 4.6661 - accuracy: 0.8485\n",
            "Epoch 177/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 9.1197 - accuracy: 0.8116\n",
            "Epoch 178/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 2.6103 - accuracy: 0.8551\n",
            "Epoch 179/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.8718 - accuracy: 0.8866\n",
            "Epoch 180/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.4962 - accuracy: 0.8840\n",
            "Epoch 181/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.2462 - accuracy: 0.8645\n",
            "Epoch 182/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.4902 - accuracy: 0.8862\n",
            "Epoch 183/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 1.7051 - accuracy: 0.8801\n",
            "Epoch 184/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.1415 - accuracy: 0.8605\n",
            "Epoch 185/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.6059 - accuracy: 0.8760\n",
            "Epoch 186/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.4678 - accuracy: 0.8746\n",
            "Epoch 187/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 4.4099 - accuracy: 0.8491\n",
            "Epoch 188/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.1366 - accuracy: 0.8758\n",
            "Epoch 189/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.4363 - accuracy: 0.8938\n",
            "Epoch 190/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.5034 - accuracy: 0.8684\n",
            "Epoch 191/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 3.4885 - accuracy: 0.8634\n",
            "Epoch 192/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 3.3198 - accuracy: 0.8367\n",
            "Epoch 193/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.6412 - accuracy: 0.8486\n",
            "Epoch 194/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.8421 - accuracy: 0.8767\n",
            "Epoch 195/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.5633 - accuracy: 0.8708\n",
            "Epoch 196/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 2.6788 - accuracy: 0.8589\n",
            "Epoch 197/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.9891 - accuracy: 0.8470\n",
            "Epoch 198/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 2.5969 - accuracy: 0.8498\n",
            "Epoch 199/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.7180 - accuracy: 0.8743\n",
            "Epoch 200/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.5504 - accuracy: 0.8834\n",
            "MSE on Training Data: 0.21533038729226806\n",
            "MSE on Test Data: 0.2710615199917707\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 0.2711 - accuracy: 0.9423\n",
            "Test loss (MSE): 0.2710614800453186\n",
            "Test accuracy: 0.942307710647583\n",
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_40 (Dense)             (None, 50)                200       \n",
            "_________________________________________________________________\n",
            "dense_41 (Dense)             (None, 50)                2550      \n",
            "_________________________________________________________________\n",
            "dense_42 (Dense)             (None, 50)                2550      \n",
            "_________________________________________________________________\n",
            "dense_43 (Dense)             (None, 50)                2550      \n",
            "_________________________________________________________________\n",
            "dense_44 (Dense)             (None, 50)                2550      \n",
            "_________________________________________________________________\n",
            "dense_45 (Dense)             (None, 50)                2550      \n",
            "_________________________________________________________________\n",
            "dense_46 (Dense)             (None, 50)                2550      \n",
            "_________________________________________________________________\n",
            "dense_47 (Dense)             (None, 50)                2550      \n",
            "_________________________________________________________________\n",
            "module_wrapper_6 (ModuleWrap (None, 50)                0         \n",
            "_________________________________________________________________\n",
            "dense_48 (Dense)             (None, 3)                 153       \n",
            "=================================================================\n",
            "Total params: 18,203\n",
            "Trainable params: 18,203\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/200\n",
            "85/85 [==============================] - 1s 3ms/step - loss: 1458.3070 - accuracy: 0.3290\n",
            "Epoch 2/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 711.2533 - accuracy: 0.2563\n",
            "Epoch 3/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 707.2326 - accuracy: 0.3578\n",
            "Epoch 4/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 705.2009 - accuracy: 0.2648\n",
            "Epoch 5/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 706.1971 - accuracy: 0.4028\n",
            "Epoch 6/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 690.8183 - accuracy: 0.3003\n",
            "Epoch 7/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 682.0967 - accuracy: 0.3995\n",
            "Epoch 8/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 661.3482 - accuracy: 0.4334\n",
            "Epoch 9/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 595.0690 - accuracy: 0.3457\n",
            "Epoch 10/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 530.6294 - accuracy: 0.2976\n",
            "Epoch 11/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 510.6538 - accuracy: 0.3636\n",
            "Epoch 12/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 508.9009 - accuracy: 0.4148\n",
            "Epoch 13/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 495.1577 - accuracy: 0.3704\n",
            "Epoch 14/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 490.5166 - accuracy: 0.3524\n",
            "Epoch 15/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 501.8793 - accuracy: 0.3991\n",
            "Epoch 16/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 479.4690 - accuracy: 0.3862\n",
            "Epoch 17/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 481.9998 - accuracy: 0.3847\n",
            "Epoch 18/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 492.1010 - accuracy: 0.3726\n",
            "Epoch 19/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 501.9601 - accuracy: 0.4354\n",
            "Epoch 20/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 474.6187 - accuracy: 0.4021\n",
            "Epoch 21/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 483.8565 - accuracy: 0.4243\n",
            "Epoch 22/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 478.5757 - accuracy: 0.4267\n",
            "Epoch 23/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 490.5807 - accuracy: 0.4384\n",
            "Epoch 24/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 485.7439 - accuracy: 0.4203\n",
            "Epoch 25/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 465.6098 - accuracy: 0.4585\n",
            "Epoch 26/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 451.6816 - accuracy: 0.4955\n",
            "Epoch 27/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 449.0894 - accuracy: 0.5051\n",
            "Epoch 28/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 424.1604 - accuracy: 0.5206\n",
            "Epoch 29/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 380.4735 - accuracy: 0.5629\n",
            "Epoch 30/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 351.1765 - accuracy: 0.5546\n",
            "Epoch 31/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 271.9838 - accuracy: 0.6506\n",
            "Epoch 32/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 233.0716 - accuracy: 0.6950\n",
            "Epoch 33/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 209.7073 - accuracy: 0.7031\n",
            "Epoch 34/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 209.3293 - accuracy: 0.7458\n",
            "Epoch 35/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 197.8574 - accuracy: 0.7511\n",
            "Epoch 36/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 184.2506 - accuracy: 0.7656\n",
            "Epoch 37/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 148.0649 - accuracy: 0.8045\n",
            "Epoch 38/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 142.9823 - accuracy: 0.7548\n",
            "Epoch 39/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 194.1121 - accuracy: 0.7444\n",
            "Epoch 40/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 133.8804 - accuracy: 0.7932\n",
            "Epoch 41/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 134.9069 - accuracy: 0.7590\n",
            "Epoch 42/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 116.7446 - accuracy: 0.8139\n",
            "Epoch 43/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 118.3220 - accuracy: 0.8010\n",
            "Epoch 44/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 110.5888 - accuracy: 0.8248\n",
            "Epoch 45/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 116.5934 - accuracy: 0.8551\n",
            "Epoch 46/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 102.9398 - accuracy: 0.8224\n",
            "Epoch 47/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 124.6153 - accuracy: 0.7833\n",
            "Epoch 48/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 73.7079 - accuracy: 0.8527\n",
            "Epoch 49/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 40.0882 - accuracy: 0.8198\n",
            "Epoch 50/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 28.9813 - accuracy: 0.8206\n",
            "Epoch 51/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 21.4907 - accuracy: 0.8076\n",
            "Epoch 52/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 16.3426 - accuracy: 0.8236\n",
            "Epoch 53/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 13.7940 - accuracy: 0.8202\n",
            "Epoch 54/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 9.9941 - accuracy: 0.8507\n",
            "Epoch 55/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 7.8513 - accuracy: 0.8615\n",
            "Epoch 56/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 6.6728 - accuracy: 0.8502\n",
            "Epoch 57/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 6.4527 - accuracy: 0.8274\n",
            "Epoch 58/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 7.1794 - accuracy: 0.8269\n",
            "Epoch 59/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 4.5954 - accuracy: 0.8791\n",
            "Epoch 60/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 6.7121 - accuracy: 0.8320\n",
            "Epoch 61/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 7.1347 - accuracy: 0.8524\n",
            "Epoch 62/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 8.3620 - accuracy: 0.8294\n",
            "Epoch 63/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 4.5780 - accuracy: 0.8470\n",
            "Epoch 64/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 4.3769 - accuracy: 0.8463\n",
            "Epoch 65/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 5.4931 - accuracy: 0.8576\n",
            "Epoch 66/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 3.4290 - accuracy: 0.8567\n",
            "Epoch 67/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 4.4443 - accuracy: 0.8678\n",
            "Epoch 68/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 9.9003 - accuracy: 0.8223\n",
            "Epoch 69/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 4.6335 - accuracy: 0.8460\n",
            "Epoch 70/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 3.4266 - accuracy: 0.8438\n",
            "Epoch 71/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 3.6021 - accuracy: 0.8537\n",
            "Epoch 72/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 3.6443 - accuracy: 0.8499\n",
            "Epoch 73/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 5.1676 - accuracy: 0.8534\n",
            "Epoch 74/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 4.2983 - accuracy: 0.8367\n",
            "Epoch 75/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 3.4220 - accuracy: 0.8582\n",
            "Epoch 76/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 4.1345 - accuracy: 0.8513\n",
            "Epoch 77/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 3.9230 - accuracy: 0.8379\n",
            "Epoch 78/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 4.5496 - accuracy: 0.8472\n",
            "Epoch 79/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 3.7645 - accuracy: 0.8479\n",
            "Epoch 80/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 2.9376 - accuracy: 0.8333\n",
            "Epoch 81/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 3.0676 - accuracy: 0.8480\n",
            "Epoch 82/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 2.1465 - accuracy: 0.8541\n",
            "Epoch 83/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 5.9715 - accuracy: 0.8173\n",
            "Epoch 84/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 4.3102 - accuracy: 0.8118\n",
            "Epoch 85/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 1.6005 - accuracy: 0.8609\n",
            "Epoch 86/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 3.9980 - accuracy: 0.8244\n",
            "Epoch 87/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 3.1040 - accuracy: 0.8327\n",
            "Epoch 88/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 2.1196 - accuracy: 0.8618\n",
            "Epoch 89/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 3.6045 - accuracy: 0.8404\n",
            "Epoch 90/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 4.8060 - accuracy: 0.8456\n",
            "Epoch 91/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 3.0636 - accuracy: 0.8545\n",
            "Epoch 92/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 8.0757 - accuracy: 0.8524\n",
            "Epoch 93/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 4.0147 - accuracy: 0.8102\n",
            "Epoch 94/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 2.2683 - accuracy: 0.8346\n",
            "Epoch 95/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 2.6520 - accuracy: 0.8617\n",
            "Epoch 96/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.7785 - accuracy: 0.8456\n",
            "Epoch 97/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 2.8111 - accuracy: 0.8582\n",
            "Epoch 98/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 4.5314 - accuracy: 0.8585\n",
            "Epoch 99/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 1.8083 - accuracy: 0.8653\n",
            "Epoch 100/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 1.4294 - accuracy: 0.8616\n",
            "Epoch 101/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.7290 - accuracy: 0.8496\n",
            "Epoch 102/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.8443 - accuracy: 0.8700\n",
            "Epoch 103/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 3.2167 - accuracy: 0.8544\n",
            "Epoch 104/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 3.2013 - accuracy: 0.8811\n",
            "Epoch 105/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 2.7297 - accuracy: 0.8624\n",
            "Epoch 106/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 4.5491 - accuracy: 0.8424\n",
            "Epoch 107/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 6.4187 - accuracy: 0.8491\n",
            "Epoch 108/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 2.1881 - accuracy: 0.8693\n",
            "Epoch 109/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 2.6899 - accuracy: 0.8621\n",
            "Epoch 110/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.0826 - accuracy: 0.8687\n",
            "Epoch 111/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 1.0222 - accuracy: 0.8673\n",
            "Epoch 112/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 2.4337 - accuracy: 0.8499\n",
            "Epoch 113/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 3.8836 - accuracy: 0.8454\n",
            "Epoch 114/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 2.1124 - accuracy: 0.8636\n",
            "Epoch 115/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 1.8596 - accuracy: 0.8534\n",
            "Epoch 116/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.9173 - accuracy: 0.8395\n",
            "Epoch 117/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.7246 - accuracy: 0.8666\n",
            "Epoch 118/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 2.2665 - accuracy: 0.8662\n",
            "Epoch 119/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 6.3283 - accuracy: 0.8553\n",
            "Epoch 120/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 7.9964 - accuracy: 0.8300\n",
            "Epoch 121/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 2.6645 - accuracy: 0.8343\n",
            "Epoch 122/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 2.7238 - accuracy: 0.8617\n",
            "Epoch 123/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 1.7642 - accuracy: 0.8607\n",
            "Epoch 124/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.7845 - accuracy: 0.8620\n",
            "Epoch 125/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 1.4642 - accuracy: 0.8734\n",
            "Epoch 126/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 1.2250 - accuracy: 0.8707\n",
            "Epoch 127/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 2.0159 - accuracy: 0.8564\n",
            "Epoch 128/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 1.3750 - accuracy: 0.8591\n",
            "Epoch 129/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 4.6896 - accuracy: 0.8463\n",
            "Epoch 130/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 1.7613 - accuracy: 0.8627\n",
            "Epoch 131/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.9585 - accuracy: 0.8950\n",
            "Epoch 132/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 1.6286 - accuracy: 0.8803\n",
            "Epoch 133/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 1.2457 - accuracy: 0.8466\n",
            "Epoch 134/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.8787 - accuracy: 0.8898\n",
            "Epoch 135/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.6829 - accuracy: 0.8680\n",
            "Epoch 136/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 1.0199 - accuracy: 0.8518\n",
            "Epoch 137/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 3.9641 - accuracy: 0.8644\n",
            "Epoch 138/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 3.6520 - accuracy: 0.8536\n",
            "Epoch 139/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 6.2569 - accuracy: 0.8414\n",
            "Epoch 140/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.2170 - accuracy: 0.8648\n",
            "Epoch 141/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.9309 - accuracy: 0.8713\n",
            "Epoch 142/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 6.6459 - accuracy: 0.8441\n",
            "Epoch 143/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 1.5370 - accuracy: 0.8962\n",
            "Epoch 144/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.2967 - accuracy: 0.8656\n",
            "Epoch 145/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 4.0394 - accuracy: 0.8381\n",
            "Epoch 146/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 1.1107 - accuracy: 0.8516\n",
            "Epoch 147/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.8928 - accuracy: 0.8677\n",
            "Epoch 148/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 2.6518 - accuracy: 0.8374\n",
            "Epoch 149/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.6811 - accuracy: 0.8178\n",
            "Epoch 150/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 1.0039 - accuracy: 0.8742\n",
            "Epoch 151/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.8244 - accuracy: 0.8511\n",
            "Epoch 152/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 3.6570 - accuracy: 0.8714\n",
            "Epoch 153/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 8.1121 - accuracy: 0.8494\n",
            "Epoch 154/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 2.4768 - accuracy: 0.8716\n",
            "Epoch 155/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.9136 - accuracy: 0.8678\n",
            "Epoch 156/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 6.1719 - accuracy: 0.8593\n",
            "Epoch 157/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.8589 - accuracy: 0.8677\n",
            "Epoch 158/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.6764 - accuracy: 0.8595\n",
            "Epoch 159/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.5939 - accuracy: 0.8496\n",
            "Epoch 160/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.4266 - accuracy: 0.8678\n",
            "Epoch 161/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.5937 - accuracy: 0.8595\n",
            "Epoch 162/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.7934 - accuracy: 0.8546\n",
            "Epoch 163/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 1.6018 - accuracy: 0.8708\n",
            "Epoch 164/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.9302 - accuracy: 0.8643\n",
            "Epoch 165/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.8066 - accuracy: 0.8308\n",
            "Epoch 166/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 2.1357 - accuracy: 0.8631\n",
            "Epoch 167/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 2.0951 - accuracy: 0.8422\n",
            "Epoch 168/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.7263 - accuracy: 0.8451\n",
            "Epoch 169/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.7619 - accuracy: 0.8540\n",
            "Epoch 170/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.6802 - accuracy: 0.8602\n",
            "Epoch 171/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 2.8486 - accuracy: 0.8717\n",
            "Epoch 172/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 4.4844 - accuracy: 0.8608\n",
            "Epoch 173/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 6.6019 - accuracy: 0.8623\n",
            "Epoch 174/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.9108 - accuracy: 0.8669\n",
            "Epoch 175/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 2.4084 - accuracy: 0.8582\n",
            "Epoch 176/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 3.3650 - accuracy: 0.8544\n",
            "Epoch 177/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.6997 - accuracy: 0.8682\n",
            "Epoch 178/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.6591 - accuracy: 0.8669\n",
            "Epoch 179/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.6417 - accuracy: 0.8719\n",
            "Epoch 180/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.7265 - accuracy: 0.8603\n",
            "Epoch 181/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.5616 - accuracy: 0.8623\n",
            "Epoch 182/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 1.5885 - accuracy: 0.8672\n",
            "Epoch 183/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 1.6490 - accuracy: 0.8821\n",
            "Epoch 184/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.5287 - accuracy: 0.8781\n",
            "Epoch 185/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.5075 - accuracy: 0.8642\n",
            "Epoch 186/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.7689 - accuracy: 0.8726\n",
            "Epoch 187/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 1.3045 - accuracy: 0.8781\n",
            "Epoch 188/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 4.2199 - accuracy: 0.8335\n",
            "Epoch 189/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.1241 - accuracy: 0.8604\n",
            "Epoch 190/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.5992 - accuracy: 0.8721\n",
            "Epoch 191/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.6986 - accuracy: 0.8381\n",
            "Epoch 192/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.5386 - accuracy: 0.8796\n",
            "Epoch 193/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.5677 - accuracy: 0.8833\n",
            "Epoch 194/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 3.3402 - accuracy: 0.8720\n",
            "Epoch 195/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.4018 - accuracy: 0.8538\n",
            "Epoch 196/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 2.7917 - accuracy: 0.8261\n",
            "Epoch 197/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.5179 - accuracy: 0.8509\n",
            "Epoch 198/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.6651 - accuracy: 0.8349\n",
            "Epoch 199/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 1.0233 - accuracy: 0.8605\n",
            "Epoch 200/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 2.0035 - accuracy: 0.8360\n",
            "MSE on Training Data: 6.799124146460645\n",
            "MSE on Test Data: 5.929707573395365\n",
            "12/12 [==============================] - 0s 3ms/step - loss: 5.9297 - accuracy: 0.7500\n",
            "Test loss (MSE): 5.9297075271606445\n",
            "Test accuracy: 0.75\n",
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_49 (Dense)             (None, 50)                200       \n",
            "_________________________________________________________________\n",
            "dense_50 (Dense)             (None, 50)                2550      \n",
            "_________________________________________________________________\n",
            "dense_51 (Dense)             (None, 50)                2550      \n",
            "_________________________________________________________________\n",
            "dense_52 (Dense)             (None, 50)                2550      \n",
            "_________________________________________________________________\n",
            "dense_53 (Dense)             (None, 50)                2550      \n",
            "_________________________________________________________________\n",
            "dense_54 (Dense)             (None, 50)                2550      \n",
            "_________________________________________________________________\n",
            "dense_55 (Dense)             (None, 50)                2550      \n",
            "_________________________________________________________________\n",
            "dense_56 (Dense)             (None, 50)                2550      \n",
            "_________________________________________________________________\n",
            "dense_57 (Dense)             (None, 50)                2550      \n",
            "_________________________________________________________________\n",
            "module_wrapper_7 (ModuleWrap (None, 50)                0         \n",
            "_________________________________________________________________\n",
            "dense_58 (Dense)             (None, 3)                 153       \n",
            "=================================================================\n",
            "Total params: 20,753\n",
            "Trainable params: 20,753\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/200\n",
            "85/85 [==============================] - 1s 4ms/step - loss: 1360.1739 - accuracy: 0.3561\n",
            "Epoch 2/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 701.8862 - accuracy: 0.3397\n",
            "Epoch 3/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 684.4000 - accuracy: 0.3718\n",
            "Epoch 4/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 671.0638 - accuracy: 0.3004\n",
            "Epoch 5/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 591.4370 - accuracy: 0.4414\n",
            "Epoch 6/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 519.4777 - accuracy: 0.3663\n",
            "Epoch 7/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 508.4245 - accuracy: 0.4052\n",
            "Epoch 8/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 503.8131 - accuracy: 0.3388\n",
            "Epoch 9/200\n",
            "54/85 [==================>...........] - ETA: 0s - loss: 515.0748 - accuracy: 0.5034"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qstoULyvQEoM"
      },
      "source": [
        "## Tree Based Methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_Z9rF6GQEoN"
      },
      "source": [
        "Trains and evaluates 2 tree based models (extra trees and random forest).\n",
        "\n",
        "Hyperparamters to tune:\n",
        "- see documentation from sklearn\n",
        "\n",
        "Note that this version of the model is not optimal. Need to play around with the above parameters more."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z5PkQYj6QEoN",
        "outputId": "74545e3e-42c7-48cb-9f5f-afbdfd2eb643"
      },
      "source": [
        "#Extra Trees baselin model\n",
        "model2 = ExtraTreesRegressor(n_estimators=102, random_state=15,max_leaf_nodes=100000,oob_score=True,bootstrap=True).fit(x_train, y_train) #fitted to shape of dat\n",
        "prediction_train_model2 = model2.predict(x_train)\n",
        "prediction_test_model2 = model2.predict(x_test)\n",
        "from sklearn.metrics import accuracy_score\n",
        "model2_MSE_train = mean_squared_error(y_train, prediction_train_model2)\n",
        "model2_MSE_test = mean_squared_error(y_test, prediction_test_model2)\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "  \n",
        "# defining parameter range\n",
        "print(model2.get_params().keys()) #geting keys for \n",
        "\n",
        "#Random Forest\n",
        "rf_reg = RandomForestRegressor(max_depth=100, random_state=0,max_leaf_nodes=1000000).fit(x_train, y_train) #fitted with respect to shape of data\n",
        "prediction_train_rf = rf_reg.predict(x_train)\n",
        "prediction_test_rf = rf_reg.predict(x_test)\n",
        "rf_MSE_train = mean_squared_error(y_train, prediction_train_rf)\n",
        "rf_MSE_test = mean_squared_error(y_test, prediction_test_rf)\n",
        "\n",
        "#Prediction Accuracy\n",
        "print(\"MSE of Extra Trees Regressor on Training Data: \" + str(model2_MSE_train))\n",
        "print(\"MSE of Extra Trees Regressor on Test Data: \" + str(model2_MSE_test) + \"\\n\")\n",
        "\n",
        "print(\"MSE of Random Forest Regressor on Training Data: \" + str(rf_MSE_train))\n",
        "print(\"MSE of Random Forest Regressor on Test Data: \" + str(rf_MSE_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['bootstrap', 'ccp_alpha', 'criterion', 'max_depth', 'max_features', 'max_leaf_nodes', 'max_samples', 'min_impurity_decrease', 'min_impurity_split', 'min_samples_leaf', 'min_samples_split', 'min_weight_fraction_leaf', 'n_estimators', 'n_jobs', 'oob_score', 'random_state', 'verbose', 'warm_start'])\n",
            "MSE of Extra Trees Regressor on Training Data: 0.011840510100037302\n",
            "MSE of Extra Trees Regressor on Test Data: 0.06838713375625692\n",
            "\n",
            "MSE of Random Forest Regressor on Training Data: 0.02072355596310749\n",
            "MSE of Random Forest Regressor on Test Data: 0.14044873059570834\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o3FsUnwtFH5L",
        "outputId": "a3f3dc7f-b46a-436c-ed57-231bb4ff82d7"
      },
      "source": [
        "# Number of trees in random forest\n",
        "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
        "# Number of features to consider at every split\n",
        "max_features = ['auto', 'sqrt']\n",
        "# Maximum number of levels in tree\n",
        "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
        "max_depth.append(None)\n",
        "# Minimum number of samples required to split a node\n",
        "min_samples_split = [2, 5,10,17,26]\n",
        "# Minimum number of samples required at each leaf node\n",
        "min_samples_leaf = [1, 2, 4,9,16,25]\n",
        "# Method of selecting samples for training each tree\n",
        "bootstrap = [True, False]\n",
        "# Create the random grid\n",
        "random_grid = {'n_estimators': n_estimators,\n",
        "               'max_features': max_features,\n",
        "               'max_depth': max_depth,\n",
        "               'min_samples_split': min_samples_split,\n",
        "               'min_samples_leaf': min_samples_leaf,\n",
        "               'bootstrap': bootstrap}\n",
        "pprint(random_grid)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'bootstrap': [True, False],\n",
            " 'max_depth': [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 110, None],\n",
            " 'max_features': ['auto', 'sqrt'],\n",
            " 'min_samples_leaf': [1, 2, 4, 9, 16, 25],\n",
            " 'min_samples_split': [2, 5, 10, 17, 26],\n",
            " 'n_estimators': [200, 400, 600, 800, 1000, 1200, 1400, 1600, 1800, 2000]}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YnrtFnLffFxr",
        "outputId": "a53ed2ab-d5d4-41e8-e884-340f23263b22"
      },
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "rf = RandomForestRegressor(max_depth=102,random_state = 15)#builds model as test\n",
        "from pprint import pprint\n",
        "# Look at parameters used by our current forest\n",
        "print('Parameters currently in use:\\n')\n",
        "pprint(rf.get_params())\n",
        "pprint(model2.get_params())\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Parameters currently in use:\n",
            "\n",
            "{'bootstrap': True,\n",
            " 'ccp_alpha': 0.0,\n",
            " 'criterion': 'mse',\n",
            " 'max_depth': 102,\n",
            " 'max_features': 'auto',\n",
            " 'max_leaf_nodes': None,\n",
            " 'max_samples': None,\n",
            " 'min_impurity_decrease': 0.0,\n",
            " 'min_impurity_split': None,\n",
            " 'min_samples_leaf': 1,\n",
            " 'min_samples_split': 2,\n",
            " 'min_weight_fraction_leaf': 0.0,\n",
            " 'n_estimators': 100,\n",
            " 'n_jobs': None,\n",
            " 'oob_score': False,\n",
            " 'random_state': 15,\n",
            " 'verbose': 0,\n",
            " 'warm_start': False}\n",
            "{'bootstrap': True,\n",
            " 'ccp_alpha': 0.0,\n",
            " 'criterion': 'mse',\n",
            " 'max_depth': None,\n",
            " 'max_features': 'auto',\n",
            " 'max_leaf_nodes': 100000,\n",
            " 'max_samples': None,\n",
            " 'min_impurity_decrease': 0.0,\n",
            " 'min_impurity_split': None,\n",
            " 'min_samples_leaf': 1,\n",
            " 'min_samples_split': 2,\n",
            " 'min_weight_fraction_leaf': 0.0,\n",
            " 'n_estimators': 102,\n",
            " 'n_jobs': None,\n",
            " 'oob_score': True,\n",
            " 'random_state': 15,\n",
            " 'verbose': 0,\n",
            " 'warm_start': False}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "asb01x7GfRYM",
        "outputId": "8c1c3e4f-26b5-4b16-c10a-b21f09ffb215"
      },
      "source": [
        "# Use the random grid to search for best hyperparameters\n",
        "# base models created\n",
        "rf = RandomForestRegressor()\n",
        "et = ExtraTreesRegressor()\n",
        "# Random search of parameters, using 3 fold cross validation, \n",
        "# Searches across 100 different combinations, and use all available cores\n",
        "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=15, n_jobs = -1)\n",
        "et_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=15, n_jobs = -1)\n",
        "# Fit the random search model\n",
        "rf_random.fit(x_train, y_train)\n",
        "et_random.fit(x_train,y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 100 candidates, totalling 300 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed:   57.5s\n",
            "[Parallel(n_jobs=-1)]: Done 158 tasks      | elapsed:  3.8min\n",
            "[Parallel(n_jobs=-1)]: Done 300 out of 300 | elapsed:  7.3min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomizedSearchCV(cv=3, error_score=nan,\n",
              "                   estimator=RandomForestRegressor(bootstrap=True,\n",
              "                                                   ccp_alpha=0.0,\n",
              "                                                   criterion='mse',\n",
              "                                                   max_depth=None,\n",
              "                                                   max_features='auto',\n",
              "                                                   max_leaf_nodes=None,\n",
              "                                                   max_samples=None,\n",
              "                                                   min_impurity_decrease=0.0,\n",
              "                                                   min_impurity_split=None,\n",
              "                                                   min_samples_leaf=1,\n",
              "                                                   min_samples_split=2,\n",
              "                                                   min_weight_fraction_leaf=0.0,\n",
              "                                                   n_estimators=100,\n",
              "                                                   n_jobs=None, oob_score=Fals...\n",
              "                   param_distributions={'bootstrap': [True, False],\n",
              "                                        'max_depth': [10, 20, 30, 40, 50, 60,\n",
              "                                                      70, 80, 90, 100, 110,\n",
              "                                                      None],\n",
              "                                        'max_features': ['auto', 'sqrt'],\n",
              "                                        'min_samples_leaf': [1, 2, 4, 9, 16,\n",
              "                                                             25],\n",
              "                                        'min_samples_split': [2, 5, 10, 17, 26],\n",
              "                                        'n_estimators': [200, 400, 600, 800,\n",
              "                                                         1000, 1200, 1400, 1600,\n",
              "                                                         1800, 2000]},\n",
              "                   pre_dispatch='2*n_jobs', random_state=15, refit=True,\n",
              "                   return_train_score=False, scoring=None, verbose=2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Mvej_Sp5p8h",
        "outputId": "8ec03340-d9f8-47bc-eebd-b384c321f4bb"
      },
      "source": [
        "rf_random.best_params_ #calling best possible params of rf "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'bootstrap': True,\n",
              " 'max_depth': 60,\n",
              " 'max_features': 'auto',\n",
              " 'min_samples_leaf': 1,\n",
              " 'min_samples_split': 2,\n",
              " 'n_estimators': 1000}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H9w9oGPe95OH",
        "outputId": "84377a1d-f84f-4ef6-d138-6807ba5f1619"
      },
      "source": [
        "rf_random.best_estimator_ #uses best estimator"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
              "                      max_depth=60, max_features='auto', max_leaf_nodes=None,\n",
              "                      max_samples=None, min_impurity_decrease=0.0,\n",
              "                      min_impurity_split=None, min_samples_leaf=1,\n",
              "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
              "                      n_estimators=1000, n_jobs=None, oob_score=False,\n",
              "                      random_state=None, verbose=0, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ROWVfArKDYN",
        "outputId": "9d0abf4b-936d-481b-f599-c8d645df3d5b"
      },
      "source": [
        "et_random.best_estimator_ #uses best estimator"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
              "                      max_depth=60, max_features='auto', max_leaf_nodes=None,\n",
              "                      max_samples=None, min_impurity_decrease=0.0,\n",
              "                      min_impurity_split=None, min_samples_leaf=1,\n",
              "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
              "                      n_estimators=1000, n_jobs=None, oob_score=False,\n",
              "                      random_state=None, verbose=0, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mPoOCy7NKLQN",
        "outputId": "43274bb1-9ca3-4913-9c78-4a3aa458fd70"
      },
      "source": [
        "et_random = ExtraTreesRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
        "                      max_depth=60, max_features='auto', max_leaf_nodes=None,\n",
        "                      max_samples=None, min_impurity_decrease=0.0,\n",
        "                      min_impurity_split=None, min_samples_leaf=1,\n",
        "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
        "                      n_estimators=1000, n_jobs=None, oob_score=False,\n",
        "                      random_state=None, verbose=0, warm_start=False) #based on best estimator\n",
        "et_random.fit(x_train,y_train) #fitted"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ExtraTreesRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
              "                    max_depth=60, max_features='auto', max_leaf_nodes=None,\n",
              "                    max_samples=None, min_impurity_decrease=0.0,\n",
              "                    min_impurity_split=None, min_samples_leaf=1,\n",
              "                    min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
              "                    n_estimators=1000, n_jobs=None, oob_score=False,\n",
              "                    random_state=None, verbose=0, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P3w81EFz976h",
        "outputId": "91f77f08-3d42-485d-f1f3-dbfbb765e9b6"
      },
      "source": [
        "rf_random = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
        "                      max_depth=60, max_features='auto', max_leaf_nodes=None,\n",
        "                      max_samples=None, min_impurity_decrease=0.0,\n",
        "                      min_impurity_split=None, min_samples_leaf=1,\n",
        "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
        "                      n_estimators=1000, n_jobs=None, oob_score=False,\n",
        "                      random_state=None, verbose=0, warm_start=False) #building random forest model based on best estimators function\n",
        "rf_random.fit(x_train,y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
              "                      max_depth=60, max_features='auto', max_leaf_nodes=None,\n",
              "                      max_samples=None, min_impurity_decrease=0.0,\n",
              "                      min_impurity_split=None, min_samples_leaf=1,\n",
              "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
              "                      n_estimators=1000, n_jobs=None, oob_score=False,\n",
              "                      random_state=None, verbose=0, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cBRzPy5i-Y4K",
        "outputId": "ff219e68-0c81-4aa0-e93c-390d44fa732d"
      },
      "source": [
        "#training rf\n",
        "prediction_train_rf_random = rf_random.predict(x_train)\n",
        "prediction_test_rf_random = rf_random.predict(x_test)\n",
        "\n",
        "rf_random_MSE_train = mean_squared_error(y_train, prediction_train_rf_random)\n",
        "rf_random_MSE_test = mean_squared_error(y_test, prediction_test_rf_random)\n",
        "#MSE hypertuned rf\n",
        "print(\"MSE of Random Forest Regressor on Training Data: \" + str(rf_random_MSE_train))\n",
        "print(\"MSE of Random Forest Regressor on Test Data: \" + str(rf_random_MSE_test))\n",
        "#training hypertuned et\n",
        "prediction_train_et_random = et_random.predict(x_train)\n",
        "prediction_test_et_random = et_random.predict(x_test)\n",
        "#MSE hypertuned et \n",
        "et_random_MSE_train = mean_squared_error(y_train, prediction_train_et_random)\n",
        "et_random_MSE_test = mean_squared_error(y_test, prediction_test_et_random)\n",
        "\n",
        "print(\"MSE of Extra Trees Regressor on Training Data: \" + str(et_random_MSE_train))\n",
        "print(\"MSE of Extra Trees Regressor on Test Data: \" + str(et_random_MSE_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MSE of Random Forest Regressor on Training Data: 0.017327658927935606\n",
            "MSE of Random Forest Regressor on Test Data: 0.09648504403735247\n",
            "MSE of Extra Trees Regressor on Training Data: 0.00481990570495884\n",
            "MSE of Extra Trees Regressor on Test Data: 0.027100764051087495\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9C2CtHLK-r2F",
        "outputId": "ee7af12f-4ef7-4d06-88ed-6575c0882c76"
      },
      "source": [
        "#building optimal model\n",
        "rf_random = RandomForestRegressor(bootstrap=True,\n",
        " max_depth=60,\n",
        " max_features='auto',\n",
        " min_samples_leaf=1,\n",
        " min_samples_split=2,\n",
        " n_estimators=1000) #from best params\n",
        "rf_random.fit(x_train,y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
              "                      max_depth=60, max_features='auto', max_leaf_nodes=None,\n",
              "                      max_samples=None, min_impurity_decrease=0.0,\n",
              "                      min_impurity_split=None, min_samples_leaf=1,\n",
              "                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
              "                      n_estimators=1000, n_jobs=None, oob_score=False,\n",
              "                      random_state=None, verbose=0, warm_start=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FsgE9j3p_RG1",
        "outputId": "491d8ee3-9643-4e32-b339-3ff3ff30ad5a"
      },
      "source": [
        "#hypertuned rf training\n",
        "prediction_train_rf_random = rf_random.predict(x_train)\n",
        "prediction_test_rf_random = rf_random.predict(x_test)\n",
        "#MSE\n",
        "rf_random_MSE_train = mean_squared_error(y_train, prediction_train_rf_random)\n",
        "rf_random_MSE_test = mean_squared_error(y_test, prediction_test_rf_random)\n",
        "#MSE Printed\n",
        "print(\"MSE of Random Forest Regressor on Training Data: \" + str(rf_random_MSE_train))\n",
        "print(\"MSE of Random Forest Regressor on Test Data: \" + str(rf_random_MSE_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MSE of Random Forest Regressor on Training Data: 0.017327658927935606\n",
            "MSE of Random Forest Regressor on Test Data: 0.09648504403735247\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VSzWRjn35q0k",
        "outputId": "533b5f36-e83c-43e8-fd3a-4f4c3500545a"
      },
      "source": [
        "def evaluate(model, x_train_input, x_test_input): #function for evaluating the performance\n",
        "    predictions = model.predict(x_train_input)\n",
        "    errors = abs(predictions - x_test_input) #error count\n",
        "    mape = 100 * np.mean(errors / x_test_input)\n",
        "    accuracy = 100 - mape\n",
        "    arr = [mape,accuracy]\n",
        "    np.int64(arr)\n",
        "    print(arr[0])\n",
        "    #print('{:15}'.format('{}'.format(arr)))\n",
        "    print('Model Performance')\n",
        "    #print(accuracy)\n",
        "    #print(mape)\n",
        "    #print('Average Error: {:0.4f} degrees.'.format(np.mean(errors))) #mean erroe\n",
        "    #print('Accuracy = {:0.2f}%.'.format(accuracy)) #gives accuracy\n",
        "    \n",
        "    return accuracy\n",
        "#fits new model    \n",
        "base_model = RandomForestRegressor(n_estimators = 102, random_state = 15)\n",
        "base_model.fit(x_train,y_train)\n",
        "base_accuracy = evaluate(base_model, x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x    inf\n",
            "y    inf\n",
            "z    inf\n",
            "dtype: float64\n",
            "Model Performance\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqnXvqwAQEoN"
      },
      "source": [
        "## Support Vector Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8dR3tbNYQEoN"
      },
      "source": [
        "Trains, evaluates and selects from support vector regression models.\n",
        "\n",
        "Hyperparamters to tune:\n",
        "- search space - the code uses grid search to optimize along listed hyperparameters. Update the params dictionary to expand or narrow the hyperparameters to search along.\n",
        "- cross validation parameter - currently uses 5 fold cross validation\n",
        "\n",
        "Probably can use the code here as a template to optimize choosing the best model for other methods."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tPG5nOS-QEoN",
        "outputId": "f9e08d05-91ca-4fa9-a583-976f0b76973c"
      },
      "source": [
        "degrees = [2,3,4,5,6,7,8]\n",
        "alphas = [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4, 1e5]\n",
        "\n",
        "params = {'estimator__C':alphas, 'estimator__kernel':['linear','poly','sigmoid','rbf'], 'estimator__degree': degrees }   #'gamma': ['scale','auto']\n",
        "svr = SVR(max_iter=500000) #creates svr\n",
        "print(MultiOutputRegressor(svr).get_params().keys()) #gets params\n",
        "param_grid2 = {'C': [0.1, 1, 10, 100, 1000], \n",
        "              'gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
        "              'kernel': ['rbf'], 'estimator_degree': degrees} #all parameters to test\n",
        "\n",
        "gs_svr = GridSearchCV(estimator=MultiOutputRegressor(svr), \n",
        "                      param_grid=params, \n",
        "                      cv=5,\n",
        "                      verbose=3,\n",
        "                      n_jobs=-1) #builds model using GridSearchCV\n",
        "\n",
        "gs_svr = gs_svr.fit(x_train,y_train) #fits to data for grid search"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['estimator__C', 'estimator__cache_size', 'estimator__coef0', 'estimator__degree', 'estimator__epsilon', 'estimator__gamma', 'estimator__kernel', 'estimator__max_iter', 'estimator__shrinking', 'estimator__tol', 'estimator__verbose', 'estimator', 'n_jobs'])\n",
            "Fitting 5 folds for each of 308 candidates, totalling 1540 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  28 tasks      | elapsed:    3.3s\n",
            "[Parallel(n_jobs=-1)]: Done 124 tasks      | elapsed:   10.7s\n",
            "[Parallel(n_jobs=-1)]: Done 284 tasks      | elapsed:   24.4s\n",
            "[Parallel(n_jobs=-1)]: Done 508 tasks      | elapsed:   54.4s\n",
            "[Parallel(n_jobs=-1)]: Done 796 tasks      | elapsed:  5.2min\n",
            "[Parallel(n_jobs=-1)]: Done 1148 tasks      | elapsed: 24.9min\n",
            "[Parallel(n_jobs=-1)]: Done 1540 out of 1540 | elapsed: 72.5min finished\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZhCLz7ThBWX",
        "outputId": "aa48cfb9-d373-4a79-83b3-2b7ea113a4cf"
      },
      "source": [
        "print(gs_svr.best_params_)\n",
        "print(gs_svr.best_estimator_)\n",
        "\n",
        "best_model = gs_svr.best_estimator_ #gets best estimate, can use for final prediction"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'estimator__C': 10000.0, 'estimator__degree': 2, 'estimator__kernel': 'rbf'}\n",
            "MultiOutputRegressor(estimator=SVR(C=10000.0, cache_size=200, coef0=0.0,\n",
            "                                   degree=2, epsilon=0.1, gamma='scale',\n",
            "                                   kernel='rbf', max_iter=500000,\n",
            "                                   shrinking=True, tol=0.001, verbose=False),\n",
            "                     n_jobs=None)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3LL7JjLBQEoO",
        "outputId": "9d039c55-b1c9-43fb-dee2-dd13e0feda3c"
      },
      "source": [
        "prediction_train_svr = best_model.predict(x_train)\n",
        "prediction_test_svr = best_model.predict(x_test)\n",
        "\n",
        "svr_MSE_train = mean_squared_error(y_train, prediction_train_svr)\n",
        "svr_MSE_test = mean_squared_error(y_test, prediction_test_svr)\n",
        "\n",
        "print(\"MSE of Best SVR on Training Data: \" + str(svr_MSE_train))\n",
        "print(\"MSE of Best SVR on Test Data: \" + str(svr_MSE_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MSE of Best SVR on Training Data: 0.04734316006366728\n",
            "MSE of Best SVR on Test Data: 0.04831445222044061\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfMRfqYyQEoO"
      },
      "source": [
        "## Summary of Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232
        },
        "id": "YerDhElQQEoO",
        "outputId": "2739a681-b2d7-45e4-dacc-b940b9a1d2df"
      },
      "source": [
        "methods = [\"DNN\", \"Extra Trees\", \"Random Forest\", \"SVR\"]\n",
        "train = [dnn_MSE_train, et_MSE_train, rf_MSE_train, svr_MSE_train]\n",
        "test = [dnn_MSE_test, et_MSE_test, rf_MSE_test, svr_MSE_test]\n",
        "d = {\"Methods\":methods, \"MSE train\":train, \"MSE test\":test}\n",
        "df_results = pd.DataFrame(d)\n",
        "df_results"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-88001d140756>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mmethods\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"DNN\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Extra Trees\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Random Forest\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"SVR\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdnn_MSE_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0met_MSE_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrf_MSE_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msvr_MSE_train\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdnn_MSE_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0met_MSE_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrf_MSE_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msvr_MSE_test\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0md\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"Methods\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mmethods\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"MSE train\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"MSE test\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'dnn_MSE_train' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_JGoClXQEoO"
      },
      "source": [
        "## Misc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8WGQrwKLQEoP"
      },
      "source": [
        "code for some metrics that dr. sideris wanted. not essential to run predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfEfjKAPQEoP",
        "outputId": "657a9c00-82dc-4742-ada9-978712e82f6f"
      },
      "source": [
        "x_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>coil1</th>\n",
              "      <th>coil2</th>\n",
              "      <th>coil3</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.002824</td>\n",
              "      <td>0.008393</td>\n",
              "      <td>0.002724</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>86</th>\n",
              "      <td>0.002926</td>\n",
              "      <td>0.003045</td>\n",
              "      <td>0.009296</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>72</th>\n",
              "      <td>0.009059</td>\n",
              "      <td>0.009267</td>\n",
              "      <td>0.005667</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>84</th>\n",
              "      <td>0.009316</td>\n",
              "      <td>0.002789</td>\n",
              "      <td>0.006905</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>63</th>\n",
              "      <td>0.003996</td>\n",
              "      <td>0.002917</td>\n",
              "      <td>0.009620</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>92</th>\n",
              "      <td>0.008527</td>\n",
              "      <td>0.008834</td>\n",
              "      <td>0.007339</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>38</th>\n",
              "      <td>0.002964</td>\n",
              "      <td>0.002929</td>\n",
              "      <td>0.004080</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50</th>\n",
              "      <td>0.002930</td>\n",
              "      <td>0.004468</td>\n",
              "      <td>0.002945</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>97</th>\n",
              "      <td>0.002835</td>\n",
              "      <td>0.008444</td>\n",
              "      <td>0.008955</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>63</th>\n",
              "      <td>0.009544</td>\n",
              "      <td>0.002848</td>\n",
              "      <td>0.005340</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>848 rows × 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       coil1     coil2     coil3\n",
              "4   0.002824  0.008393  0.002724\n",
              "86  0.002926  0.003045  0.009296\n",
              "72  0.009059  0.009267  0.005667\n",
              "84  0.009316  0.002789  0.006905\n",
              "63  0.003996  0.002917  0.009620\n",
              "..       ...       ...       ...\n",
              "92  0.008527  0.008834  0.007339\n",
              "38  0.002964  0.002929  0.004080\n",
              "50  0.002930  0.004468  0.002945\n",
              "97  0.002835  0.008444  0.008955\n",
              "63  0.009544  0.002848  0.005340\n",
              "\n",
              "[848 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9qDx25bQEoP",
        "outputId": "8df045cf-a912-4398-f40b-3e0cc1422ddc"
      },
      "source": [
        "y_train['x']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4      2.400016\n",
              "86    51.600344\n",
              "72     0.000000\n",
              "84    60.000000\n",
              "63    60.000000\n",
              "        ...    \n",
              "92     0.000000\n",
              "38    60.000000\n",
              "50    30.000200\n",
              "97     0.000000\n",
              "63    60.000000\n",
              "Name: x, Length: 848, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9MLULVMQQEoP",
        "outputId": "99c55e76-cb30-4842-8ab1-fec8564cebff"
      },
      "source": [
        "df1 = pd.DataFrame(prediction_test_et, columns = ['x','y','z'])\n",
        "df1.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>x</th>\n",
              "      <th>y</th>\n",
              "      <th>z</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>60.000000</td>\n",
              "      <td>21.684145</td>\n",
              "      <td>60.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>40.164268</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>60.000000</td>\n",
              "      <td>25.980173</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>60.000000</td>\n",
              "      <td>29.640198</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>19.140128</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>60.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "           x          y          z\n",
              "0  60.000000  21.684145  60.000000\n",
              "1  40.164268   0.000000   0.000000\n",
              "2  60.000000  25.980173   0.000000\n",
              "3   0.000000  60.000000  29.640198\n",
              "4  19.140128   0.000000  60.000000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fjh5aMqqQEoP"
      },
      "source": [
        "df1 = pd.DataFrame(prediction_test_et, columns = ['x','y','z'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tgenSe_SQEoQ"
      },
      "source": [
        "y_test.to_csv(path_or_buf='labels.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fViA5tEXQEoQ"
      },
      "source": [
        "df1.to_csv(path_or_buf='predictions.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5XSAMnUZQEoQ",
        "outputId": "d3d89abe-7fa6-4937-ebbd-ef9334f384ae"
      },
      "source": [
        "prediction_test_et"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[60.        , 21.68414456, 60.        ],\n",
              "       [40.16426776,  0.        ,  0.        ],\n",
              "       [60.        , 25.9801732 ,  0.        ],\n",
              "       ...,\n",
              "       [60.        , 60.        , 14.9700998 ],\n",
              "       [60.        , 20.61613744, 60.        ],\n",
              "       [60.        , 48.70232468, 60.        ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "26Bn-aqIQEoQ",
        "outputId": "834823eb-3b5a-4b39-c8a5-f74469a40a80"
      },
      "source": [
        "y_test_array = y_test.to_numpy()\n",
        "y_test_array"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[60.      , 21.600144, 60.      ],\n",
              "       [40.200268,  0.      ,  0.      ],\n",
              "       [60.      , 26.400176,  0.      ],\n",
              "       ...,\n",
              "       [60.      , 60.      , 15.0001  ],\n",
              "       [60.      , 20.400136, 60.      ],\n",
              "       [60.      , 48.600324, 60.      ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0VdFGPRZQEoQ",
        "outputId": "e2bdb936-8bd1-4188-eaac-95b91cb501df"
      },
      "source": [
        "from numpy import linalg as LA\n",
        "from statistics import mean\n",
        "\n",
        "error = []\n",
        "x1 = prediction_test_et\n",
        "x2 = y_test_array\n",
        "\n",
        "labels1 = []\n",
        "labels2 = []\n",
        "\n",
        "for i in range(len(y_test_array)):\n",
        "    p1 = x1[i]\n",
        "    p2 = x2[i]\n",
        "\n",
        "    vector = [p1[0] - p2[0], p1[1] - p2[1], p1[2] - p2[2]]\n",
        "    n = LA.norm(vector)\n",
        "    error.append(n)\n",
        "    \n",
        "error.sort()\n",
        "mean(error)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.16229530624795396"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESbDmc2-i0L8"
      },
      "source": [
        "# ***Building Scatter Plot*** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w69NmsdxfKn3"
      },
      "source": [
        "# importing mplot3d toolkits\n",
        "from mpl_toolkits import mplot3d\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "IjcKIYNuhO-P",
        "outputId": "4e0e99b0-7a91-4dfd-f5be-1e97ce06f90e"
      },
      "source": [
        "#plot of random forest output and actual\n",
        "plt.scatter(x_test, y_test, color='blue') #base\n",
        "plt.scatter(x_test,prediction_test_rf,color=\"red\") #rf\n",
        "plt.title('Test Data vs. Random Forest')\n",
        "plt.xlabel('Field Strength (A/m)')\n",
        "plt.ylabel('Vertical Distance (mm)')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'Vertical Distance (mm)')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "aflPSlmckE2L",
        "outputId": "3332c7b4-33ef-4427-ad04-fbe38bb409a2"
      },
      "source": [
        "#extra trees vs actual\n",
        "plt.scatter(x_test, y_test, color='blue') #base\n",
        "plt.scatter(x_test,prediction_test_model2,color=\"purple\") #et\n",
        "plt.title('Test Data vs. Extra Trees')\n",
        "plt.xlabel('Field Strength (A/m)')\n",
        "plt.ylabel('Vertical Distance (mm)')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'Vertical Distance (mm)')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "KRuYyXAu5KNG",
        "outputId": "71688fa8-103c-4c35-ab33-a6b6f3d32164"
      },
      "source": [
        "#svr vs actual\n",
        "plt.scatter(x_test, y_test, color='blue') #base\n",
        "plt.scatter(x_test,prediction_test_svr,color=\"orange\") #svr\n",
        "plt.title('Test Data vs. SVR')\n",
        "plt.xlabel('Field Strength (A/m)')\n",
        "plt.ylabel('Vertical Distance (mm)')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'Vertical Distance (mm)')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAEWCAYAAABhffzLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO2de3xcZZn4v88k6SVJIWlaEVqbVEEWFuViuViqVC5e6gV0tSsMtHIxIrBb5bdesxdxN7vq7oLd5VorUOigIruIsnVFrhIqlSKwIiiCTUpbgSRNaJO0aZLz/P44Z5KZyTkzJ5eTzOX5fj7nMzPved/zPufMzHPe87zP+zyiqhiGYRilQ2y6BTAMwzCmFlP8hmEYJYYpfsMwjBLDFL9hGEaJYYrfMAyjxDDFbxiGUWKY4jcMwygxTPEbU4aI9KRsjojsS/kcH8fxHhaRS7LsbxARTenjVRG5V0TOGkMfnxKRlrHKFjUicrGI/E5E9nrntUlE5ojIl0XkFz7154nIARE5xjunIe+a7BGRZ0TkQ9NxHsb0YIrfmDJUtTq5AduBD6eUJSLsusbr81jg58DdIvKpCPuLFBE5Dfhn4FxVnQMcBfzA270RWCoiizOafRL4jao+633+pXdNaoDrge+LSE300hv5gCl+Y9oRkZg3Un1JRDpF5E4RmevtmyUiG73ybhF5QkQOEZFm4F3Atd7I9dpc/ajqK6q6Fvga8E0RiXl9JPveKyLPichHvfKjgBuBd3p9dHvlHxSRp7zR8ssi8rUs5/Z86mhaRMpFpF1ETgg6txCX7ERcxf2Ud167VXWDqu5V1R3Ag8AFGW1WAbf5XBMHuB2oAo4I0bdRBJjiN/KBvwLOAU4DDgO6gOu8fauBg4E3AXXApcA+VW0CHgWu8J4YrhhDf/8NvAE40vv8Eu5N5GDgKmCjiByqqs97/f3S6yM5Iu7FVaQ1wAeBz4rIOQF9fQ84N+Xz+4AOVf110LmFkH8L8D4RuUpEThWRmRn7N5Ci+EXkSOA44I7MA4lIGXAhMAC0hejbKAJM8Rv5wKVAk6ruUNV+3BH5x0WkHFch1QGHq+qQqj6pqnsm2N8u73UugKr+UFV3qaqjqj8A/gCcFNRYVR9W1d949f8PV7mfFlD9DuAjIlLpfT7Pqw/jPDdVfRT4GHAC8D9Ap4hc7SlxgLuBQ0Rkqfd5FfBTVW1POcwp3hPMfuDfgPNV9bVcfRvFgSl+Ix+ox7W7d3vK6HlgCDgE1wzxM1wb9C4R+ZaIVEywvwXe624AEVklIk+n9H8MMC+osYicLCIPeSab13FvXL71VfVF73w+7Cn/jzAy8h73uanqT1X1w7g3r7OBTwGXePv6gB8Cq0REgDijzTyPe08wtcCPcZ94jBLBFL+RD7wMfEBVa1K2Waq6U1UHVPUqVT0aWAp8CHcECzDe0LIfBV4Dfi8i9cB3gCuAOk8ZPgtIlj7uwFWWb1LVg3HnAcSnXpKkueds4DnvZkCOcwuF99TxAK5d/5iUXRuAlcBZwBzgJwHte4DPAheIyPFj6dsoXEzxG/nAjUCzp4QRkfkicrb3/j0i8jbPjLEH1zzieO1eBd4cthNvUvgK4B+Ar3gTm1W4yr3dq3Mh6Qr0VWChiMxIKZsD7FbV/SJyEq75JhvfB96Lq2CH7ew5zi3beZwtIp8UkVpxOQnX1PR4SrVHgW5gHfB9VT0QdDxV3Q2sB/4+V99GcWCK38gH1uKOoO8Tkb24Cuxkb98bgbtwFePzwCO4JpJku4+LSJeI/EeW43eLSC/wG2AF8AlVvRlAVZ8D/h34Ja6SfxvwWErbB4HfAq+ISIdXdhnwdU/WvwfuzHZyqvon7/hLGXG7zHpuInKjiNwYcMgu4NO4cxF7cF04/zXVJVbdRBu34ZrRRnnz+PBtYIWIvD1EXaPAEUvEYhiGUVrYiN8wDKPEMMVvGIZRYpjiNwzDKDFM8RuGYZQY5dMtQBjmzZunDQ0N0y2GYRhGQfHkk092qOr8zPKCUPwNDQ1s3bp1usUwDMMoKETEN/6SmXoMwzBKDFP8hmEYJYYpfsMwjBLDFL9hGEaJYYrfMAyjxCgIrx4jT9mWgGeaoK8NpAx0CCrr4dhmd/8zTdC3HSoXuWWL4z5tA/YbRtSk/gZnzHVjtA50jvyWZ9R5Zbsn/hvNs997pEHavOTN63HD3CpwEfB73AiFDUArsFJVu7IdZ8mSJToed85EAuY/eyZnHfPAmNtmQ/EPvq7Aru56/tR7OCcc9hAxyRlh15fBIaG8LPv3oioMOmVUlA2mlXfsrWNm+X7mzO716oGkCKte25iMPn5qXUdjgfIPOWW0dR/Jm+c+l3bs1ONARr8p3e3dX8XM8gFmVhxI258paxAHBssoL1Ni4uCo0NtfRfWsXrZ3LOJnv1nBuafcNnz+fucl4tDVWwdAbdVuevZVUjVrX9r5Ohqjt3821bP62NW9iNaaZtqI09QE27fDokXQ3Axx7797898nOGNeE2+qa0Mzrl2yz869bp91czpxnDJisaHA7yLJkFPGU39azptrnmZuVefw9RsYmkVt1W52dS/ivlebeXwLfHVFE4vmbWdn1yLaat2bb0N3E4fVbB8+h2XxOC2JxKjywV2PseyN6yiLDQ33vbO7frhNEC2JBPVdTSyobRs+p51d9by0bwWHV25K6yPb9QP3/9rUBEsXJLjxws8Mf4eOxrj/2fdwQsPTzJuTvAbV9A/MpG5OZ85rmI0wv7ckY1WVmXoi8P+wr4oDQ7OYW7077XuaDETkSVVdMqo8YsW/AXhUVdd78cwrga/ixjL/hoh8GahV1S9lO854FH8iAce8/Oe8/U3+yilKwiqwQiZfzzEquXr7K/nsreu4/Rcjf8jKSli3Dvp/nyD+lovSbmSTSa5zGhgqpyw2SCylzv6BCgRJk6m3v5InOlZz4rwNVM3sS2tfHhv07aO3v5KnZqzzVUQtiQTHH2hMO1aQzL39lWx4dDUrjt3Eonnb2d6xiKt+1MyZl8SJx93/6/3rE3zrL9cwb07nKFny9fcWBdmu+ViZcsUvIgcDTwNv1pROROT3wHJV/ZOIHAo8rKpHBh0Hxqf4//rsBGtXnl8yPxYjelrb61n8uda0svp6eOIr85jvjUTzncGhMsrLhnJXTGFHVz0LL28dXX5dAwtrw+dndzJG5r39lXzlR+v4j3vi/PXZCf7lHP+bSCkSdM3HSpDij3JydzFuVqNbROQpEVkvIlXAIV5iCoBXcPOqjkJEGkVkq4hsbW9v96uSlStPbzKlb0wqi+ra2PbtBoY2xtj27QbOXZpg+3aYV10YSh9IM+WE5bCDt/uX1/iXB5Fpjqma2ceVpzcB7v/VlP4IQdd8sohS8ZcDJwA3qOrxQC/w5dQK3pOA7yOHqq5T1SWqumT+/FGhJnJSPy/8SMQwQiFCw/w2YqI0zG/jO5c0csWHE7nb5RFDTtmY2+x6fZF/ebd/+VhYNG972qvhEnTNJ4soFf8OYIeqbvE+34V7I3jVM/Hgvb4WRefj+YFPFqWQ1KwUzjGVTDMFuCPWf17ZRM9AXaR9j+daOwr9AzPSynr7K2l5pZHe/sq08oGh8sA+evsraa1p9t3XWtOMo/6P1ZnHC6rXx6K0VyP7NZ8sIlP8qvoK8LKIJO33ZwDP4eZWXe2VrQbuiaL/2DgeaTNJepoE7cv87DiubW7rzjNwHAn1h032ka2vXH2HOX62PpOb46S+Hy1/ct/gUBlbd55Bx966tPZ7+qpo31OH49VxgvoZpRTCy6gK/QNlWa9vZr+p5zXkxHAUOnvq2Lu/KrDfISfGnn1VOI6wo6se8X8wpVq2M+e0tQw4FYHyJ/sP871kfh5yhK07z6CzZ+Ra79lXRWdPHY4j7O6tG9W3o8ILfJYnym9mR1f98Dk8NWMdy79wPU/NWJdWviV2K4/s+iyDQ2VpcifbBE0yLovHeYFLRyl1R4Xnu85I6+MFLmVQ0284g1pJ9VJXwVUvDb6JZJLrNx3UJtuxxrtNFsn/Ra5rPllE7dVzHK475wzgj8CFuDebO4FFQBuuO+fubMcZz+RuT6KBav/AdOGZeRic8C34VSMMpdgfyyph8WrYtSm7X26mn/DgfnBG3AypqIMla0fa/eoyeGmd60OMQHkVDPam+Bh7/sSHrRjpu2IuDO0FJ9WjRAAd8alvf2zkuFIGb2mEk67Pff5R+R77HRfG3le2dQRh5Qx7jj9qcPvJpLIezmnNfZww55z6vY7lek+3j3jY/nPU+13iMt7KjVldM1Whi6OZe8Rpvr9pTUigi7FfueMIsfPH53ad9vsbJgax2e7/PPmbzMWMOnjH2ki+s2lx55wsxuXHvy2Bs/mC8fn3npfRZrr/WLnId/mKgW0J/wHASevsWk8iLYkEp5LdG08VuvrqqKyEWZK+uCrI0yjIm2myvGfylSDFX7wrdxfHeWHzYxypN4zNu6esyvdYef3nznf5ioHk9bUbbKQsi8d5+F8f47TDgv+3IgwvZgPcEfevGgF33qG2P90ttLe/kufa38mSBQ+MWkT1Yt8KFkZxInlOUcfq+bP49TzGRoacMZzmSTdFJ5BR2CyOu2ad8xz31ZR+JCz/wvU8suuzY7OhD/XBM00si8dHzWE80bGa4w99eNSNRAQOr9w0qbIXCsVr6knBScRymnxUQQ46Gj7823H3YxjG5JFtZbAfqiBLN6bdkHMdw3Egdn7+68DxMh0LuPKGMP7G+wdmmNI3jDwidfQeZnwqAoObG935GI+G7uwLwxydPrfv6aQkFH82f2Nw3c9u/cXFJAprLY5hFD3L4nEWXt7KY2wctf7Aj3Lpo2dz0/DnXKuLJ8PtuxApCcW/LB5n2+6jAkcNMVEuOu27bLnTNL9h5CNjGf1XMqLscz3t7+qunwzxCo7iV/zbEuy/rTowhHCSmRUH+LsPrJk6uQzDGBPJ0f8tv88++t/eMaLsW2uaA+tOxQrZfKW4Ff+2BIObG5lV3hvKpXNegURYNIxS5qKvx/nBtnW076kbNfrv7a/k6gdHlHm6l0/66vMnOlZHvkI2Xylqxd+zuYlyGVvEvxYz9BtG3nPR1+PcN6eDi9ZvpLXddd1sba/nitvWcfLKdGW+LB6ntaaZfQOViLiTwOVlQ7z7sBvRhMBd8+CH8+COmLtCe1vx64DiXcBFuq0vDCLQ0NUElOYowDAKCXewHmd5Uzwwq1eShu4mqmrTB4HDLt4H0heDDW5udBVjEa/TKOoRf6qtLyxRx8E2DGPyiMehtdX1x29t9Vf6MLbcAZmeQcVIUSv+qx8MntgJIuo42IZhTD1jzR0wVmtBoVHUiv/klXGuuG3dsA1wcCj7Yo1k7A7DMIqLbN49fozHWlBIFLXij8fhzEviLL+mlfJVDjc92JjVB1gE3nbwnVMnoGEYU0Kmd09mTohUVOGRF4t7AFjUih/SbYAfPG5TTrfOuVWd5tljGIXItoTrlXNHzPXSuSvdU2fZUli4wB3gZVMDIvCh44s7eFtRe/VkEiavp3n2GEYB4q3ZGXbfHsgI27zlIoaGlDIZCLWmp3am2fiLhr39c0PVM88ewygscq7ZcQ5QJgOhj7e9YxGJBDQ0QCzmvhaTIaBkFH8iAeLsD1V35xg9AAzDmF4m0wunt7+Sf97UzP3rEzz8+QYGb4/x8OcbuH99omiUf8ko/i13JpgzuzdnPVXY9HRxT+wYRrExUS8cR8VdC+Ct/gW4dlUjDfPbiInSML+Na1c1Fk0gx5JR/Fee3hTKticCH1tyZ9Hc2Q2jFLj6weasHnv9AzPYP1CRVuao4Ci8vLueS2+7nfJVyvJrWjnzkjhfXTE6jn/VzD6uPL04FnaVjOIPM7GbZN6czqK5sxtGKXDyyjgde+t89w0OlXHhupu5eN0taXF9Ll5/O99DedMVraz7WRzHgY3NCU7rbqB+3uiE7TA2PZLPlIzi7xsMN7EL7qi/WO7shlEKxOPwk11rRy3S6u2vZNWNG7jn6TgHHzuypic5sk8N8ZBM07iwti3QOtBHccz/lYzi7+8fW/1iubMbRqlw0dfTE623ddTz6fXr+N7mOLNnw6mnZo/rkytN46BWUr20OOL3R+rHLyKtwF5gCBhU1SUiMhf4AdAAtAIrVbUrSjkAaqt2j6l+H4uojkgWwzCiwY2vHyeRgMavQJ+nxzs7obHRfR8cyM3fvKMKUlVP+bHNRROxcypG/O9R1eNSMr1/GXhAVY8AHvA+R85YgjQ5KuzAPHsMo1BpaoKzj0uw7dsNDG2Mse3bDZx9XII1WZLsOY5/LK8hpwzOaS0apQ/TY+o5G9jgvd8AnDMVneYK0pTqERAT5U0HNljoBsMoUE5dkOA7l6S7Y2687AK+9qHLAj32ghKvx6T4ErJHrfgVuE9EnhQR70GLQ1T1T977V4BDIpYBGAnSNOT4n3LmZE7VzD4aum2C1zAKkW+eN9peHxPlsrNuDPTYC0q8vuv14kvIHrXiX6aqJwAfAC4XkXen7lRVxb05jEJEGkVkq4hsbW9vnxxh4nFEsjj7ZmChGwyjMFkQkHglJhrosednFSjWhOyRKn5V3em9vgbcDZwEvCoihwJ4r68FtF2nqktUdcn8+fMnLEtLIsGO6xoQ//uML5aUxTAKE6kK/u8Geeylh24WdnTV80THahq6m3ASMXZc11A05t/IFL+IVInInOR74L3As8CPgdVetdXAPVHJkCSMf24mlpTFMAqYY5tR9f+zZ/PFXxaPs/DyVmLnO7TWNHPivA0srHXnCRbWtnH8gcaiUP5RjvgPAVpE5BngV8D/qOr/At8AzhKRPwBnep8jJZd/rh8icHhlccfkNoyiZXEcOeLSUcrfUaGStlCjdz+9USxzf5H58avqH4Fjfco7gTOi6tePsSRaTmtnNn7DKFxOuh6Zfyo804T2tqEIMW+Ob2FtG7X9jbQkkr7/ownSG8WgF0pi5e5YEy0n6eoLH+bBMIw8ZHEczmllZ3f9sNJP4jd6T84FOolYoKmos7fw9UJJKP4X+1ZkjdwXRPXMvUVhzzOMUifM6D11LjAmSlnM8T/YOHRJvlESiv/wyuy5doecmO+NYWbFgaKw5xlGqRP01J/quRd2LrBuTmfOOvlOSSj+XDb+mDiBN/FisOcZRqkTxkc/7FygqhR8vo6SUPy5bPzu04D/I4H58htG4ePno//UjHVpE7th5wJjogWfryOnV4+IvBM4H3gXcCiwD9cf/3+Ajar6eqQSTgKtNc0s0POzmntiom4UvpQ6yRHBwuhFNAwjYpKROwEWelsqrTXN1PY35jT3jOTrKNygbVlH/CLyU+AS4GfA+3EV/9HA3wKzgHtE5CNRCzlRlsXj7O71z86TSlLpq0JnT92oEYFhGMVL5lPB3v1VgU4hhZ6vI5ep5wJVvVhVf6yqu1R1UFV7VPXXqvrvqroc2DwFck6Y56tGZ+cJQgRmVlWb0jeMEiO5cnez3E5MNNBKsLtnbkHb+bMqflXtSP0sIgeJyNzk5lcnX0m/m5PTvbMK/6QMhmEUF6m++8kVvbk8fKpn7SloO79oCAd3EfkMcBWwnxEvVlXVN0co2zBLlizRrVu3TtrxEgn48L5qDqrsDazjqBBbentRJV8wDCOdpO9+qpLv7a+kckZfzrhere31NKxpjVbACSIiT6YkwRomrFfP3wDHqGqDqi72tilR+pNNSyLB+3vnMWd2sNIHd7KXZ8yH3zCKmaB4PGFYNK9wrQJhY/W8BIwtylkeMnx3rw53KtrbFuDkaRhGMTDeOF7gpmosVH/4sHJ/BdgsIjeJyH8ktygFi4KxRukcCsjBaRhGcRDkux8mfHssNlSwE7xhFf9NwIPA48CTKVtBMda7e1lADk7DMIoDvxW9YeN6qca4f32iIJV/WMVfoapXquotqrohuUUqWQSMNUqnjfgNo7hJevu176kbVvhhkzWVxRyuXdXII7cVnuYPq/h/6uXAPTTTnbOQaK1pxgkItepHTGzEbxjFzrJ4nN7+6tAKP5WqmX00f3RNwY36wyr+c/Hs/IyYeSbPv3KKWBaPc/Ojl+KEDasqAtsK7Bs1DGPMTGQl7rw5nQU36g+l+FNcOBcXujvn7Hddz+qbNoay48VE6dlsLp2GUexky8ObCxEKbtQfSvGLSJmIfERE/lpErkxuUQsXBfE4vP8z8dC5FCop7JgchmHkpnppM4MaLqSLH/PmdBbUSt6wpp6fAJ8C6oA5KVtBsmDXZaH987d3WFhmwyh6FscpX7puXJn6YCRiZ6GM+sMq/oWq+jFV/QdVvSq5RSpZRDz8r5dx2mE3hJrI6R+YwdUPNueuaBhG4bM4zs7u+nE3r5/XVjDunWPx6nlvpJJMEcveuC707L2irFwZrTyGYeQPfn79qWR7IhCBa1c18tMb81/5h1X8jwN3i8g+EdkjIntFZE+UgkXFWBZlzaoYYFmVTe4aRqmQ9OvPpuA79tYF7q+a2cetjavzfuQfVvFfDbwTqFTVg1R1jqoeFKFc+UOfTe4aRimRKw/HDx5fiSwN9gwsLxvi2lWNeT3ZG1bxvww8q2FiOGfgeQQ9JSL3ep8Xi8gWEXlRRH4gIjPGesyJMNYT6FGb3DWMUqMf/4x9IvCZ09fB4ji7+4PnA6pm9nnpGfOTsIr/j8DDIvKVcbhzrgGeT/n8TeAaVT0c6AIuDi/uxHHGEIZh/0AFX73TJncNo9SYtXRt4Ig+aS6+96kVWU1C9Xkctjms4t8GPADMYAzunCKyEPggsN77LMDpwF1elQ3AOWMTeWLEstj4M79EQegsiPxihmFMKovjaIB6dDRGIgGnHb4pq6OIIrTkqaE/VDz+Cbhufhv4IiM3iTqgW1UHvc87gAV+DUWkEWgEWLRo8swtu7rrWVjrfyfO/BJnVhzgm+c1AZaFyzBKjRf4DEfqaNdvxxEeSSQ4d1X2+b+YKA1d+ak/so74ReQ7IvK2gH1VInKRiPielYh8CHhNVccVvllV16nqElVdMn/+/PEcwpf7Xm0e0yKNBRNI1GAYRmGSSMA315/qqysqyodo/ugauvpzD0gPOzg/9UcuU891wN+JyPMi8kMRuV5EbhaRR3EDts1hxGyTyanAR0SkFfg+rolnLVAjIsknjYXAzomexFh4+pmx1d85xlDOhmEUNokE/PTGBNeuaiQWoCHnzenk3qdW5Azz0NmTn0GMsyp+VX1aVVcCJ+LeBB4FfgxcoqrHqupaVe0PaPsVVV2oqg3AJ4EHVTUOPAR83Ku2Grhnck4lHFee3hR+AZfCl+6wyV3DKBUSCbh/fYJbG1dnzdYn4tr4y5euw9HCy9sRNjpnj6o+rKrfU9UfqervJ9Dnl4ArReRFXJv/dydwrDEzlgTJqvDYzvyzzxmGEQ1b7nRH+uVluRd6Lpq3HRbHiS3dEGg+njenMy8neKckV7B30/iQ9/6PqnqSqh6uqp8IemKIirG4c4pAsw34DaNkuPL08Hm5h0M5L45n9fs//kBj3in/Qk0SP27Gmkc3xyI+wzCKiLAWgUGtpHrpyKhw1tK1gfb+qpl9NHTn12KuMSl+ERl/wOo8oZfxR98zDKN4SSTcBOpBuOYcgcp6ype6q3eHyRHWOd+8e8ImYlkqIs8Bv/M+Hysi10cqWUSMJeFCb39VxNIYhpEvbLkzQUycwP07u+vhPAfOaU1X+kmyhHV2NAZ3xOBHDXmRzjXsiP8a4H1AJ4CqPgO8OyqhImUMCReGmBW9PIZh5AXZPP4cFVprck/4+YV1VsWbLFboa4NfNU678g9t6lHVlzOKxmYszycWxxkKMck7Z2bnFAhjGEY+EBRbRxVe4NKcUTthJKzzjq56HEcYHCobfTMZ6oNnptfmHypkA/CyiCwFVEQqGB14reAIM8nrOGWlN/ttGCVISyLBUgTxid/bTx1/Fg9v2XZvEO5NQhL+GkR7t4dO/xoFYfXapcDluHF1dgLHeZ8LljAj/mwB3QzDKB4aupuIyWil76gwa+la3zYtiQQ7rmvAScTYcV2Dr8tm0Mr/6Y4IEHYBV4eqxlX1EFV9g6qer6oFbQcJo9S3d5gHkGGUAocFxeRS9Z3IbUkkOP5AIwtr24iJsrC2zddf/0t3jLb59/ZXTntEgLBePRtEpCblc62I3BydWNGTzW0LLNG6YZQCyVG7n4kHYNfr/oO/hu7RC738/PUf2xnn0+vX0b7HTdeoCn39s6mqnhz5x0tYU8/bVbU7+UFVu4DjoxEpeloS2d22VOE7D13MyStt9ZZhFCupo3Y/b579AxWBnjxBTwiZ/vrNzVBeDpUz9yHiruSdf1An13xyelfzhlX8MRGpTX4QkbmEnxjOOxq6swdqE4Gzl2yyVbuGUcT4jdpT6e0/KNCTZ1eAjb6rLz0aZzwOzZ8I93QwlYRV/P8O/FJE/lFE/gk3JPO3ohMrWgLteSksCEjWYhhGcZBLD9RW7g7c11rTTP/A6HThVTP3jBrJL6gNejqYPh0TdnL3NuAvgFeBV4CPqertUQoWJUF361Sm09XKMIzoyaUHHI0FmmOWxeP09I/OPjurYmDUSD6wH5FpW8g1Fjf13wH/jRuPv0dECjZDid/qOsMwSotceqC8bChrZM3aKv8ngkw7f2tNM46OHkrGROnZPD3mnrBePX+FO9r/OXAv8D/ea0GSXF03OFR4CRQMw5gcUlfZJj1uMslmiw8aye96Pb18WTwe6DVURVvgGoAoCTviXwMcqap/rqpvV9W3qerboxQsapbF46y+MTiBgmEYxc+yeJyFl7fST12gw0dQZE2/J4be/kpfT6C2gDVBIgSuAYiSsIr/ZeD1KAWZDiy7lmEYbEswk+D1qJkj+CSZcXl2dNXz1Ix1vp5AVz+Y3axUNbOPo3rXjF32cRJW8f8ReFhEviIiVya3KAWbCpqboWOvf+acoHLDMIqLns3B7t2qZI3KmXxiiJ3vsHBFM8uqmnzDL5+8Ms4Vt62jtb0+0Mowt6qT3yUum8CZhCes4t+Oa9+fAcxJ2QqeOx9fOeqLUIVNv105PQIZhjGlVBHsVqkQKion2xIMbm50wy574ZcHN4+EX47H4cxL4iy/pjWr2eet3Dglnj6iBWDkXrJkiW7dunVSj7m8MQ8AACAASURBVJlIwP3rE6y/eBVlsdGreHsG6qhe3TGpfRqGkX8M3l4emFy9tb2ehjWtOY/Rk2igWkbfQHq0nup4evuWRIJTOT/wKWO/1jErPjm6R0SeVNUlmeVhvXrmi8i/isgmEXkwuU2KZNPEvkcv47uXXOCr9AGqygs6Bp1hGAFkRtUMCtioSuh4XZX4TwD7lS+Lx9ndG2xKnkln5KP+sKaeBK4f/2LgKqAVeCIimSKnJZHg4nfd4BuG1TCM4sUvqmbQck1HhZUhLb7bO/wngIPKn69a6+vbD67JJ2r//rCKv05VvwsMqOojqnoRcHqEckXK0X1rssbqMQyjOPGLzxMT9VXCZTHlFMKlSfTz2untrwx8YlgWj3Pzo5cGTvQGPUFMFmEV/4D3+icR+aCIHA/MzdZARGaJyK9E5BkR+a2IXOWVLxaRLSLyooj8QERGB7yImNrK3GYc8+oxjOIjOD6P+i7oLJe+UKPvVK8dxxFa2+u54rZ1WSP8zn7X9YF6JuhJYbIIq/j/SUQOBv4f8DfAeuBzOdr0A6er6rG4GbveLyKnAN8ErlHVw4Eu4OJxSR4hqq63j2EYxcXOLn+FumN3PbGA+b4wo+9Ur53yVQ7Lr2nlzEviWSP8xuPQdPfaMT0pTBZhFX+Xqr6uqs+q6ntU9R1AcOg6QF16vI8V3qa4JqK7vPINwDnjkHtC5BrNi8CH37FpiqQxDGOq+NmzK3zdt1945XAcx18dhh19x+PQ2gqO476G8QI9bdXYnxQmg7CK/z9DlqUhImUi8jTwGu46gJeAblUd9KrswM3j69e2UUS2isjW9vb2kGKG456nR/vuZ7IwIJSqYRiFy5lHbRo1vycCpx/9oK9LZ9Sj7/E8KUwGWZOpiMg7gaXA/IyVugcBOSOcqeoQcJyXtvFu4M/CCqaq64B14Prxh20XhvcdM/rLz6SzZy7zJ7NTwzCmnUXz/Ad0fh5+yTSJxx07OX23JBI0dDdxWM12dnUvorWmmWVxV8lPddKnXCP+GUA17g0idcXuHuDjYTvx0jY+BLwTqBGR5A1nIbBzjDJPmDBJVmaW72caM6MZhhEBfYNZfVLSSKZJ/MvFEw+gFjY5+1SRVfF7rptXAaeo6lXe+38E1qvqH7K19RZ91XjvZwNnAc/j3gCSN43VwD0TPIcx4zi5wzHPmd3LljtN8xtGMdHfP/Y2k5EmMWxy9qkirI3/X0TkIBGpAp4FnhORL+RocyjwkIj8H+5ir5+r6r3Al4ArReRFoA747jhlHzdBK/VSEYErT5++nJiGYUw+QclTchEUmjl0+5DJ2aeKsIr/aFXdg+uB81PcFbwXZGugqv+nqsd78fuPUdWve+V/VNWTVPVwVf2Eqo7jHjwxdnX7B0nKZNE8y7trGMVEmLSrvu0CQjNPtN9dry8ikYCGBojF3NepsP6EVfwVIlKBq/h/rKoDEJBSpgAIm3oxjEnIMIzCwe+/v3+gwjdxehJVeLFvxaT329tfyX2vNnP/+gQPf76BwdtjPPz5Bu5fn4hc+YdV/DfhxuepAn4hIvW4E7wFybJ4nO9tWZ3TpTOMScgwjMJhWTzOEx2rGRwqQxUGh8p4/LVLeKL85uEUjJmIwOGVE1vXE5S05eln4NpVjTTMdyd9G+a3ce2qRh65LVrNP+6wzCJSnuKPHylRhGVuXdtAw/zsppz2PXXMv9RCMxtGsZD0rkmdaO3trxzOnOUkYr6unY4jxM73X9k7EYL0UPueOu6b0zFhN89xhWUWkfO91yszN+CvJybS9BLkz5uGBXIzjKIiyLvmqN417LiuITAp+kRt/EEE6aF5czoj9SrMZeqp8l7nBGwFS1d/7i+yrspi8htGMRHkXTO3qpOFtW2+CzuDEqhPBn3466GovQqzrtxV1Zu816sik2CauPepFaw65YasK3gdLQs9CWIYRv6zq3uRF4M/HT89oApDThlPdKxm+ReiWVpbvbQZ3eyfjSuUVWKc5NRrIvIeEfkvL7Tyb0XkLhFZHplEU8Rph+cO21Bmk7uGUVT4edcETXOKQHnZECfO2xDdCtvFcfrxDxoZ9DQwGeSy8X8QuBm4FzgPiAObgJtFZGL+TdNMmLvpzpD+/oZhFAZ+3jWOZh//JucAomLW0rUMavrNaFArqV4aXXC4XCP+LwDnqOotqvqMqj6tqjfj+vN/KTKpIiaRgJ592f34+wdm8KU7oo2JbRjG1LMsHmfh5a3EznfcV8ntrTO3qjPSUX/50nVQWQ8IVNa7nxdHF7ktl+J/o6o+k1moqv8HHBKNSNGz5c4Ec2b3Zq0jMkjdvCkSyDCMaWMoxEJNEaKNq7M4Due0wnkOLb3N7NjUNJwMPoobTi7Fn007ZtececyVpzfltO/PKHf455UWq8cwip2wCzWnIq7OVEXxzKX43yIiP/bZfgK8eVIlmULCzpZXiyVjMYxioyWRYMd1DcMj6q7ecPm1J+rLHyYmz1RF8czqzgmcnWXfv02mIFNJ78Bc5szI7aPfo4uongJ5DMOYGoZX7ta6ynVhbRv9AzPYP1DBrIqB4Xqq6S6eSV/+hePsN5HAi8nTxKJ529nesYir1jcD6dm2piqKZ5h4/IHbpEoyhQwMhKgzWMZX77TJXcMoJvxG1DMrDtDbf1Cap8/WnWekxfN5omM1yyYQP2HLnQnfmDyZq3ODonh29YVPIBOGklyfVFOZOyb3kMbotDA9hlFUBI2oayt3D3v6tNY0c/T8X1JeNjRpvvxXnu5vwslcndta08z+gYpR7atn7p1UO39JKv4wMblnVQzwzfNsctcwiolscfGTRGFnD5pXzCxfFo/T23/QqHozKw5Mqp2/JBX/i30rcoZkBlgQMDowDKMwCYqL31rTDNsS8KMGFtT4R+2diJ09KNevX3lQlrDJtPNnndz1vHcCVaSqfmTSJJlCDq/MHa4BQKqiWzJtGMbUsywepyUBDV1NHHbwdna9vojWmmaWLYXBzY2US1+gbtj1+qJxT+7290P1aAuOW57ZT0A8oYn0n0kur56C9dzJxmEBd/Q0yirhWJvcNYxiw52kdSdqF3pbT6KBaukLbLN/oGJCXj1Bo/han/nG1ppmavtH5wyYSP+Z5IrOWbCeO9nINdhXBTkp2iXThmHkD5VkN6OUlQnLlo7/+N19c5nrE+a9e99cMo09gU8lE83KkkKuET8AInIE8C/A0cCsZLmqFuwirmyogpjSN4zSYFsCx4kRKwtewVsRO0DP5iaqx6kXKnzMPNnK/Z5KJpOwk7u3ADcAg8B7gNuAjZMsy9SwLbdLVGdPuJV8hmEUONsSrm0/i9JPkuupIBtVFf6mnqDyqAmr+Ger6gO4OXrbVPVrwAejEys6ejbnjtNjGEZpsH/zGsqz2PZT2d4xfmePoLYTOeZECKv4+0UkBvxBRK4QkY8yejK6IAhz166bMz13YcMwppBtCWbiH7ol0927t7+Sqx8cv7PH1Q/6u5FmHjMzjlBUoaDDKv41QCVugvV3AOcDq7M1EJE3ichDIvKcl7lrjVc+V0R+LiJ/8F5rJ3ICYyXMHbZ3YHKXRxuGkX9ke/rv2FtH+546VN2bwL7+2axcOf6+Tl4Z54rb1tHa7oaFaG2v54rb1nHyypE5g6mKzAkhFb+qPqGqPaq6Q1UvVNW/UNXHczQbBP6fqh4NnAJcLiJHA18GHlDVI4AHvM9TxtUPNudcvBUmlo9hGIVN0NO/Ktzz9EqqZu1DxA3WNu+gTk6hMdQcoR/xOJx5SZzl17RSvsph+TWtnHnJSIC2lkSCU5zVUxKZE0Iqfm9kXpPyuVZEfpatjar+SVV/7b3fCzwPLMCN+LnBq7YBN5vXlHHyynjOVGs1s83UYxjFTude/yf7jr11fPSkTVTOSFfC5dJHz+bcSjgo/HI8Dq2t4Djua6rSP/5A8ARzFHkAwpp65qlqd/KDqnYBbwjbiYg0AMcDW4BDVPVP3q5XCMjkJSKNIrJVRLa2t7eH7Son8Tg88Ox7so76Jxp32zCM/KYlkeCg2XtHle8fqKDp7rXUzvRXtrnmCEfCLzcweHuMhz/fwP3rE76x95P4xQZKJQp9FFbxOyIy3LuI1JMllEMqIlIN/BfwOVXdk7pPVTXoOKq6TlWXqOqS+fPnhxQzHCcsfjrQtjcct8MwjKKlobuJmRUHRpXv3XcQp62Kj8sLJ5GAn94YLvxyapugiKEQnT4Kq/ibgBYRuV1ENgK/AL6Sq5GIVOAq/YSq/rdX/KqIHOrtPxR4bexij5+WRIK66uCZ/KdmrJvUFXKGYeQfQWFb6uZ0Eo/D/c+PDuSo6pb7kRzp39rob6fPDL+c2sZx/NXw4FBZZPoo7OTu/wInAD8Avg+8Q1Wz2vhFRIDvAs+r6tUpu37MiEfQauCesQo9ERq6g2fywyRdNgyjsGlJJALDtjieDnjfMaMDOYq45X4kE60E2en9wjLve/QyvnvJBb5tevsreTy2IbJBaFbFLyJ/5r2eACwCdnnbIq8sG6cCFwCni8jT3rYC+AZwloj8ATjT+zxlZAvQVl42FJn7lGEY009LIsGSwQsDB38xcZXwglp/80tQqHa/RCup9JFuImpJJLjoXTcSk9GW7ihH+klyxeq5EmgE/t1nnwKnBzVU1RaC46GdEUq6CHCcsqwxOapm9tHQ1UQyToZhGMVDQ3cTs2qD/bV3vV7PQsYeGjko0QrAoFZSvTTdTt/Q3USs1n+aNBZzIjc354rO2ei9/YCq7k/dJyKzfJrkPbFY7pgcUbhPGYYx/WSbSFVlOPTxi30rWFBzQ9qTgapb7qf4+1hENaNvFI6WUb50dKTfbHK4x4qWsJO7m0OW5T27uutz1zF3TsMoSrKlXXU0NjzS9kvWJOKW+1G9tJlBTQ/JMKiVxJZu8A3vHiSHozLq6SAKctn43ygi7wBmi8jxInKCty3HDeFQcLTWZF+5m0y4YBhG8ZEt7argDL8Pmgs87OCAOcLFcXdkX1kPCFTW+470k/ilgHRUeIFLpyQPSC4b//uAT+GGg/53Rmz2e4CvRifW9DHRhAuGYeQv2dKuJu37EDwX6GhZ8Gh5cTy00p6KZCvZyGXj3yAitwPnqmrBu7q0JBKcOHgREpD8ACaecMEwjPwlyLaeat+H4LnAMHOEYYk62Uo2ctr4VdUBPj8FskRO0Gq9TKp8JmkMwyh8gmzru3vrhkfbLYnEsD//6Pa55wgLgbCTu/eLyN94oZbnJrdIJYuAbDPpqeQK4mYYRmHiZ1vv7a/k+aq1QPaAacUUziWshvtL4HLcUA1PetvWqISKimwz+qnExMldyTCMgmNZPM5TM9axo8uNi7+jqz5tsVRQwLSpWFQ1lYRKtq6qi6MWZCrw8801DKO0yGZbD7IKxCT6RVVTSdh4/JUi8rciss77fISIfCha0SafbDP6qXTstWTrhlGKBFkFim1tT1hTzy3AASDp6LgT+KdIJIqQMDb+/QMVrLl97RRIYxhGvhE0B1Astv0kYRX/W1T1W8AAgKr2ERyHJ2/JZeNXhfUPXcLmncXzSGcYRnhyzQEUC6Fs/MABEZmNlzRFRN4C9EcmVUTksvGLwEdO2ETtWVMrl2EY+cN0+tdPFWFH/F8D/hd4k4gkcJOkfzEqoaLibQffmdPG/6a67RTZzd0wjCmgJZFgx3UNOIkYO65ryOvw7rli9VwnIqeq6n3Ax3DDN3wPWKKqD0cv3uTRkkgwt8o/81YqvRTXJI5hGNGT9P9fWOumXFxY25bXuT1yjfhfAP5NRFqBLwG7VPVeVe2IXLJJJlvmrST7Byr46p3FNYljGMb4CDuCb0kkOMXxT7nY0D065WI+kFXxq+paVX0ncBrQCdwsIr8TkX8QkbdOiYSTRBiPHkHoLLhbmmEYk03YEXy2lb6Qv7k9wubcbVPVb6rq8cC5wDnA85FKNsmEWbU7s+IA3zwvP+/QhmFMHX4reKtm9nFU75qc9VLJV///sAu4ykXkw97E7k+B3+Pa/AuG1ppm9g9kCcvpEZRT0zCM0iHIQjC3qjNt1J/NkpDP/v+5JnfPEpGbgR3Ap4H/wfXp/6Sq3jMVAk4Wy+JxHn/tkqxJWAB2hoznYxhG8RJkIRAhzW4fVC/fY/vkGvF/BTfF4lGq+hFVvUNVe6dArkmnJZHgxHkbsk7wqsKX7sjPO7RhGBGyLQE/aoA7YvCjBnpqgjN1LahpS6vnt9L38diGvFX6kDsRy+lTJUjUHNW7hqrqYFscuKnPHrNVu4ZRWmxLMLi5kXLx9ENfG4ezgb37qzho9uhxrjt41OF6L85YTXXXpmnJpDVewq7cLWhaEglODeHDL6I024DfMEqHbQmczaspl3SvnHLpA63CUSEmwfbhculjoW6i+vJWoHBW+kaWcUREbhaR10Tk2ZSyuSLycxH5g/daG1X/qYTx4QfY2VVvq3YNo0CY8EpZb6QfE39XzDmze7Mq/SSVFJ5DSJSppm4F3p9R9mXgAVU9Ajfsw5cj7H+Yw2pyp1JUhbZaG+4bRiEwGStlezY3jZh3fAibt2N7R+E5hESm+FX1F8DujOKzgQ3e+w246wEiJyh/ZioKeW+XMwzDJcjPfiwrZbON1HN5/yXp7a/k6gcLb8A41cllD1HVP3nvXwEOCaooIo0islVEtra3t0+o01jM/1Eurb8J9WAYxlQS5D+fc6VsiveO4/irv8GhMjp7sidjUoXW9nquuG0dJ68svAHjtGUVV1XFC/McsH+dqi5R1SXz58+fUF+7uusn1N4wjPxiXJmyPJs+fW2AUl42NGpk39tfyafWbeB31WvpH6r0PUxvfyXx6zey/JpWzrwkXpDzglOt+F8VkUMBvNfXpqJTv6w6hmEULve9OvZMWT2/WDPKpi/ijvAdR4ZH8B+4NE4bcW59ZDWDQ2WouiN8xxkZ5X/wsjitrRSk0oepV/w/BlZ771cDU7L6N5lVJ5vdztFpe/gxDGMMJBLw6KPQ1z97WCm376njB9uCV8q2JBJUlfu7dMdiDuWrnLQR/JY7E5x/6gbKy4YQcW8Q+wYq+du7mgt2lJ9KlO6c3wN+CRwpIjtE5GLgG8BZIvIH4Ezv85SwLB5nKMskr+BMlSiGYUyALXcmuHZVI/MP6hxWypUz9/H0M8Ftsrl0b+9Y5I7mW0dG8Fee7j95/E8fL44gjlF69ZyrqoeqaoWqLlTV76pqp6qeoapHqOqZqprp9RMp2SZ5O3vqyNOcCYZhpBCklK88PVgpB00Gq+LrlbNonn/9+nlt3L8+UfC6oiTsG8mFHtk8d2qqXmfLnQX+bRpGCVA/z39dzqKAcgieDO7YW+frldMXkIlPBK5d1VjwuqLoFX/qQo9sCzIqygazjhgMw8gPgky22dbr+Dl49PZX8pNda33t9dVLmxlUf4eQXE8XhUDRx+pp6G6iqjZ7cLYkQY93hmHkD0Em22R5SyJBQ3cTh9VsZ1f3SNC0lgQ0dDWlBVO76OsBs7SL45QDuvl83wFjoeuKolf8YVIuJunqX0T2ZRuGYUw3fdRTzWizTh/1PO094ScHewtr26jtb6QlkVyZ7yr6UMHUFsfp3dwU0Nciqid2GtNK0Zt6unrnhqqnCvc+tSJiaQzDmCh+ZphBraR6abMbfj1EKIewAd6y9VXIFLXib0kkqJq5J1RdETjt8E0RS2QYxoRZHKd86TqorAcEKuspX7qOls1uakQ/UkM5jCnAW0BfLC5sR/6iVvwN3U3MqhgIXb/Q7XaGUTIsjsM5rXCeA+e00rIZTnFWBzpwpIZyCArwtrCjiYYGRrtqZvRV6Eofilzxj8W+D66N3zCMwiI5gi8v85/0VSUtlEOQXqif18bDn28oCj/9XBS14g/y3fXDbPyGkd/42eUTCXhT5+gRfCq7e+vSQjlkTaQ+v40bPnUh7+2ZN/4ELwVAUSv+sQRnMxu/YeQvQXb51x+4jEV1wQu3evsreb5qbVpZLr0wq2KA+XM6x53gpRAoasWfDM62o6s+VGKFbCv/DMOYPoLs8peecWOgXX9wqIynZowO3JbUC509daH0wlgTvBQCRa34wf2SF17eioZItRImU5dhGFNPkF0+KCdub38lj8c2ZM2qN6tiX+j0ijkTvBQYRa34U22CQdl2UgmTqcswjKlnrPN1yZF+kL++3xNE1v6zJXgpQIp25W5Lxgq+mJdtJ9sd3l0RaBhGvtFa00xtf2Oasg76P/dSP6z0g1bxLh2Dx18ywUvOlb4FRNGO+P3u6CIw5MR87XpDWlHwq/EMo1hJ2uXb947Y5f2Ufuqq2qB5gaN614SyAKjCjq5633mCQqdoFf9hNf4TtSIOsnQjzEiJylNRR9nSW4piYYZhFCvL4nF691f7KnxVRq2qDZoXmFvVGejzn35MYeHlrUWn9KGITT2OU0bM58sV3Ih7u3vreL5qY1F+qYZRqPhF1gSGy+rn+U/mKoKc05pWtqt7EQtrRw8Aw07o7np9UVGZd1Ip2hF/0ERtMlVbXXUnJw5eVHT+uYZRqPj56i8ZvJATBy8aLgtS2n6JU/z89YPcNzPLcyVuL3SKVvHv6q7PWWdmxYGi8881jELFzyY/q2KAmRUHsrYLipaZuo7HcYQdXfXs7vUPvL67ty6tXjHa9VMpWlOPnxeAH8Xmn2sYhcpYYmu5Hj0ClYsoP7Y5cH4uMwZ/SyLB8Rl6Ibm6N6noQ8XqL3CKdsQfdtVusfnnGkahMZITO8QyWo+d3fXjipbp9xRQ7KN7P4p2xA8jd/uWRIIlAxeOCtHcPzCj6PxzDaOQyPS1D8NE/erHnImrCCnaEX8qy+Jxtpbfwt79VagyvM0oP8AyOR/uELj/TLfytgT8qAHuiLmv26Zx8jefZDGMCBjrCtqg+DtBhM20VWqIholSNM0sWbJEt27dOvaG2xLwTBP0bYeyShjqHUfvAodfCiddD7+6DF5aB5riMSRl7ufKeshiaxwl15Nr4ICXLaiiDpasHWm7LQFb18CAfzahMVNRB/UrYdcm91pULnJlbX8MXrwJcLyKZYB6n2MQmw1O30j91HMbvrZtI9cg81rAyPX3O0YYUr/DGXNd8QY6R/qqqHN9dA/sTtm/GyrmjpSH6Tu1n4nWz/z+ZtTBO9bmvh65jpnreufzOpSAc3MSscB4O5kMDJWzJXbriNLP8Z39LnEZR3JDmieQKgwxg3LJPmHsi5TBWxpdXVAgiMiTqrpkVPl0KH4ReT+wFlfTrFfVb2SrPx7F/9J//jlvnvtcaJ/dbKjCgaEyZpQNZT3eWC5l5nEy206G3JnHz/wDjKWfZP2d3fX01KzgcDZQLsEjNUddvevXJ7heFB29h/DWNzwXKOO+gRnMrjgwad+hu3bDVcBH9a4ZTtPn148qOBpDxKHL8wSprdrNru5Fvuc/4FSwd99B1Fa6xwz6flPLB5wZ7OmbQ23Vbrr75nLQ7G7KU9yQVaGnv4qqmb2jrqXf+Tka49E/fYbyw04d9nvv2V9J1cx9xMRhyCmj5ZVGln/h+jR/+a7euVSU7WfOrJGBUfLcHaeMWGyIrt46ZpT1Uz2rZ1S/Sbl299bxm9dXcnjlJg6raRtuqyqjXDEHtZIXWc0RehNlMYcwpP5+9g/Morx8iIrYQNp+hbRwjFH8j8ZUVxi+Dru662mtaZ7S+YS8UfwiUga8AJwF7ACeAM5V1eeC2oxV8T/xrTNZsuCBSf/SDRfH+yNPlFyxk6Jg/0AFZTGlomxw3MeYrPOPAlXXHFJRHpyNauvOMzh6/i/HZGIZS/9jGUyU2n+0t79ySieTgxT/dNj4TwJeVNU/quoB4PvA2ZPZgSn9aJkspTcd39GsioEJKX2YvPOPAhEClX5y/5IFD0Si9JPHj6JusZAvsf2nQ/EvAF5O+bzDK0tDRBpFZKuIbG1vb58y4QzDMKIkH9YO5a1Xj6quU9Ulqrpk/vz50y2OkYGjJThcS6HUz3+ilPL1y4e1Q9Oh+HcCb0r5vNArmzS27jxjTJMwYck8ZtItdLpIdU1N3Sbz+H709ldy4wOX0tpej+O4NuWkbdlxwv+po7x2QcfeP1DBwNDElq/09ldy0/D5C+176ugfmBGq/+S+Pfuq2T9QEbpNZj1VcBzx/U0ODAZnkkva+MPmoh4ruc5BFdr31HHTA5eOumapdcL8lh1HcPLX6uZLvsQAmg7F/wRwhIgsFpEZwCeBH09mByd+8f5h5T9ehZhZ31HY/Mczhv/sre31xK/fyPk3bKR9T12gEp7olqlUB4fKcJTh/mPna9oWv34je/ZVpygH6OufMfx5T18V7XvqcByhY28de/pG1jbsOzCLIU+ZDA6V8fPfnJGm3J2U+OQHn3E9y69ppewCZdaFg8TOd1/LLlAuve12Xt5dj5Mid/ueOvbsG+mrs6eOF147Ou1cHSf93Pfsq6Jjb92wcg26zu176tLOcciJ8fNnz6CtY7TsW8tvYUvs1uF8q0H9pLbr7Kmjs6cubaXnQd75l69yOPEbHSReunl4NejLu+u58YHPutdOGb6mSVnj12/k7f+4lzteuiVtBek9z3827Rz37qt2+025jq3t9Zx/g/u9z/iUw3U//2zab+SRXZ9lS9mG4ePu2Vc1nIMiuf/EL96ftnq1syf9u0lew9R+O3vq2Jvyu/L7zjp76nhk12fd4+rI9Useo63D/c2e+I0ODjrjep4ovznte0hen/O933X8+o3D/7c9fVVpfe3dV82tL9zOpbdtHL7Oyf5SzyH1d5G6DQyJb7njUzbezR0Ejf7v5MMq4ely51wBfBvXnfNmVc16Cxy3H38q2xKw5TPghPDlD/J7z2c/acMwjAyCvHqmJWSDqm4CNk1pp4vj/op7rAt3DMMwCpyijtUTiqAbgmEYRpGSt149hmEYRjSY4jcMwygxTPEbhmGUGKb4DcMwSgxT/IZhGCVGQcTjF5F2oG2Ku50HdExxn2OlEGSEwpCzEGSEwpCzm8IA2wAACT5JREFUEGSEwpBzojLWq+qomDcFofinAxHZ6rfwIZ8oBBmhMOQsBBmhMOQsBBmhMOSMSkYz9RiGYZQYpvgNwzBKDFP8waybbgFCUAgyQmHIWQgyQmHIWQgyQmHIGYmMZuM3DMMoMWzEbxiGUWKY4jcMwygxSkLxi8j7ReT3IvKiiHzZZ/9MEfmBt3+LiDSk7PuKV/57EXmfVzZLRH4lIs+IyG9F5Kp8lDNlX5mIPCUi9+ajjCLSKiK/EZGnRWSCiRcilbNGRO4Skd+JyPMi8s58klFEjvSuYXLbIyKfm4iMUcjplX/e++88KyLfE5FZeSjjGk++307ndRSROhF5SER6ROTajDbv8P47L4rIf4iETGGvqkW94SZ7eQl4MzADeAY4OqPOZcCN3vtPAj/w3h/t1Z8JLPaOUwYIUO3VqQC2AKfkm5wp7a4E7gDuzUcZgVZgXj5/596+DcAl3vsZQE2+yZhx/FdwF/Dk1bUEFgDbgNlevTuBT+WZjMcAzwKVuOHr7wcOnyYZq4BlwKXAtRltfgWcgquTfgp8IIw8pTDiPwl4UVX/qKoHgO8DZ2fUORv3Tw1wF3CGd+c8G/i+qvar6jbgReAkdenx6ld420RnySddTgARWQh8EFg/QfkikzECJl1OETkYeDfwXQBVPaCq3fkkY0bbM4CXVHWiK96jkrMcmC0i5bjKdVeeyXgUsEVV+1R1EHgE+Nh0yKiqvaraAuxPrSwihwIHqerj6t4FbgPOCSNMKSj+BcDLKZ93eGW+dbwv+XWgLltbcc0nTwOvAT9X1S35KCduissvAs4E5YtSRgXuE5EnRaQxT+VcDLQDt4hrNlsvIlV5JmMqnwS+NwH5IpNTVXcC/wZsB/4EvK6q9+WTjLij/Xd5ZpZKYAXwpmmSMdsxd+Q4pi+loPgjQVWHVPU4YCHuiPCY6ZYpExH5EPCaqj453bLkYJmqngB8ALhcRN493QL5UA6cANygqscDvcAoO20+ICIzgI8AP5xuWfwQkVrc0e1i4DCgSkTOn16p0lHV54FvAvcB/ws8DQxNq1CTSCko/p2k36kXemW+dbxHz4OBzjBtvcf9h4D356GcpwIfEZFW3EfL00VkY57JiDcCRFVfA+5m4iagKOTcAexIebK7C/dGkE8yJvkA8GtVfXUC8kUp55nANlVtV9UB4L+BpXkmI6r6XVV9h6q+G+gCXpgmGbMdc2GOY/oz3smKQtlwR2p/xB1dJCdV/jyjzuWkT6rc6b3/c9Infv6IO0kzH29iD5gNPAp8KN/kzGi7nIlP7kZxLauAOToyibUZeH++yentexQ40nv/NeBf801Gb//3gQvz+P9zMvBbXNu+4Nq1/yqfZPT2vcF7XQT8jolN5o9bxpT9nyL35O6KUPJMxo8j3zdc+9wLuLPqTV7Z14GPeO9n4T4Wv+hdyDentG3y2v0eb8YceDvwFPB/uLbAv89HOTOOvZwJKv6IruWbvT/BM54yaMrXawkcB2z1vvcfAbV5KGMV7ijx4Hz9/3jlV+Eq02eB24GZeSjjo8Bz3m/zjGm+jq3AbqAH9+nzaK98iXcNXwKuxYvGkGuzkA2GYRglRinY+A3DMIwUTPEbhmGUGKb4DcMwSgxT/IZhGCWGKX7DMIwSwxS/MW2IyFBGNMkGEdkcol1PQPmtIvJxn/JTvGiHT4sbVfNrXvlyEZnIwqHQiMjnvKX/yc++5+DT7hwR+fuMsqdF5Ps+dU8Rke+MQaZ/E5HTw9Y3iofy6RbAKGn2qRv2IpUoFPEGYKWqPiMiZcCRXvlyXL/oUTcbESlXN17KZPE5YCPQN8Z2X8QNv5CU6yjcRVDvEpEqVe1NqfsB3PACYflP4DvAg2OUyShwTPEbeYWI9Khqtff+C8BK3FWVd6vqP2TUFVzldRZucKsDAYd9A24wMFR1CHjOi3V+KTDkxYn5K+Bi3AiIxwOPich1wHW4K7X7gE+r6u9E5FZgD+7imTcCX1TVu0QkhruI5nRPngHgZtx4NIcBD4lIh6q+x5O/GfgQsA84WzNCLIjIW4F+Ve1IKT4Xd8HTUbjxbu5I2XcGcLWIfAo3SmMVcARuQLQZwAVAP+7qzt2q2uYFIXujqr4ScO2MIsRMPcZ0MjvFzHN36g4ReS+u0joJd8XsO3yCt30Ud/R+NLCK4KeFa4Dfi8jdIvIZEZmlqq3AjcA1qnqcqj7q1V0ILFXVK3ETXf+Vqr4D+Bvg+pRjHoobI/1DwDe8so8BDZ48FwDvBFDV/8ANO/yepNLHVcqPq+qxwC+AT/vIfSrw64yyv8QNyfA93JsAACIyDxhQ1de9omM8eU4EmoE+dYPL/dK7Vkl+7fVjlBA24jemEz9TT5L3ettT3udq3BvBL1LqvBv4njeK3yUiviYLVf26iCS8452HqzCXB/T7Q1UdEpFq3BvJD1OSGs1MqfcjVXVwnx4O8cqWee0d4BUReSigD3CfTpIZ0Z7EfWrJ5FDcUNAAiMgSoENVt4vITuBmEZmrqru9c0sNbfyQqu4F9orI68BPvPLf4IYcSfIa7tOIUUKY4jfyFQH+RVVvmoyDqepLwA3e5Ge7iATFOU/azGNAd5YbU3+GrGNlQEfipQzh/1/chxuhMcm5wJ950VYBDgL+AtdO/wHg6gD5nJTPTkZfs7x+jBLCTD1GvvIz4CJv5I2ILBCRN2TU+QXwl15SnEOB92QexGv7wZRcpEfgKtpuYC8wx6+Nqu4BtonIJ7xjiIgcm0Pmx4C/EJGY9xSwPGVfYF9ZeB443Os/hjvf8TZVbVDVBlwb/7neub0dN2b8WHkrbpAvo4QwxW/kJepmZLoD+KWI/AY3/n2m4rwb+ANuBMXbcO3XflyAa+N/GndiNO6Zh34CfNSbY3iXT7s4cLGIJCOHZqbKy+S/cCMnPofrwfNr3CxK4M4X/G8O808mvwCO9xT7u4CdqrorY//RuCamp1KeIEIhIhW4N5ZJSXBvFA4WndMwJhERqVbVHs+U9Cvg1Il4zIjIWuAnqnp/ljp/i5vPdZRvf45jfxQ4QVX/brzyGYWJKX7DmERE5GGgBtd98luqeusEj3cIcLKq/nji0o069idw80VPJGm8UYCY4jcMwygxzMZvGIZRYpjiNwzDKDFM8RuGYZQYpvgNwzBKDFP8hmEYJcb/B0z5sUwPQZ/rAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}